{"cells":[{"cell_type":"markdown","source":["# Least Squares Regression\n\nThis notebook explains how to implement least squares regression using PySpark Map-Reduce."],"metadata":{}},{"cell_type":"markdown","source":["Spark exposes two interfaces to data:\n\n1. An [RDD](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD) interface which represents a collection of rows which can be any python object.\n1. A [dataframe](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html) interface which is similar to Pandas (but with less functionality) and built on top of the RDD interface\n\nToday Spark users are encouraged to try to use the dataframe interface which provides additional system performance optimizations. However, in the homework and in this notebook we will use a bit of both to get some exposure the the low-level side of distributed data processing."],"metadata":{}},{"cell_type":"markdown","source":["## Loading the Diamond Dataset\n\nWe begin by loading data describing diamonds.  We will load this data using the dataframe interface to Spark."],"metadata":{}},{"cell_type":"code","source":["diamonds = (\n  sqlContext.read.format('csv')\n    .options(header='true', inferSchema='true')\n    .load('/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv')\n    .cache()\n  )"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["We can take a look at a table summarizing this data:"],"metadata":{}},{"cell_type":"code","source":["display(diamonds)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Data Preperation"],"metadata":{}},{"cell_type":"markdown","source":["For this notebook we will examine a subset of the columns and only expensive diamonds.  In the following we use the Dataframe API to subselect the data."],"metadata":{}},{"cell_type":"code","source":["data = (\n  diamonds\n    .where(diamonds['price'] > 1000)\n    .select(['cut', 'color', 'carat', 'clarity', 'price'])\n)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(data.sample(False, 0.01).select(['carat', 'price']))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["# Spark RDD Exercise"],"metadata":{}},{"cell_type":"markdown","source":["Let's count the number of diamonds in each cut:"],"metadata":{}},{"cell_type":"code","source":["# We can access the underlying RDD for any dataframe\nrdd = data.rdd\nrdd"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Spark is implemented in Scala (which runs in Java) and thus much of the backend of Spark is backed by java functionality (hence `JavaToPython`).  We will now try a few functions on the `rdd`.  This RDD is composed of [`Row` objects](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.Row) that behave like python dictionaries and even have a convenient `toDict()` function that returns a native Python dictionary.  They keys in the row correspond to the columns in the dataframe."],"metadata":{}},{"cell_type":"code","source":["rdd.count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["The following code snippet will take an RDD and transform each row into a tuple consisting of the String corresponding to the color of the diamond in that row and the integer 1.  The [`reduceByKey`](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.reduceByKey) function assums that the input RDD consists of tuples of `(key, value)` pairs. It takes a function that takes two values (or intermediate value) and combines them to produce a new value.   Notice that nothing happens when we run this cell:"],"metadata":{}},{"cell_type":"code","source":["counts_rdd = rdd.map(lambda row: (row['color'], 1)).reduceByKey(lambda a, b: a + b)\ncounts_rdd"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["To compute an actual value we instead must invoke an action like [`collect()`](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.collect) or [`count()`](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.count)."],"metadata":{}},{"cell_type":"code","source":["counts_rdd.collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["# Least Squares Regression in Map-Reduce\n\nTo implement least squares regression in map-reduce we recall that the the optimal parameter value is given by the **normal equation**:\n\n$$\n\\hat{\\theta} = \\left(X^TX\\right)^{-1} X^T Y\n$$\n\nWe can compute a single entry in the \\\\(X^T X\\\\) matrix:\n\n$$\n\\left(X^T X\\right)_{i,j} \n= \\sum_{k=1}^n X_{i,k} X_{k,j}\n$$"],"metadata":{}},{"cell_type":"markdown","source":["In the following we specify the set of features and which features are categorical."],"metadata":{}},{"cell_type":"code","source":["features = ['cut', 'color', 'carat', 'clarity']\ncategorical = set(['cut', 'color', 'clarity'])\nlabel = 'price'"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["We then define two map functions:\n1. `xtx_map`: which computes the entries of the $$ X^T X = \\sum_{i=1}^n x_i x_i^T$$ matrix\n1. `xty_map`: which computes the entries of the $$ X^T Y = \\sum_{i=1}^n x_i y_i$$ matrix\n\nThese functions are slightly more complex because they implement one hot encoding of the categorical features.  Try to figure out how they work. A few observations:\n1. They reach return multiple values using the python `yield` syntax\n1. The values correspond to entries in the matrix \\\\(x x^T\\\\) and \\\\(x y\\\\) respectively.\n1. Categorical features (e.g., `cut=\"Ideal\"`) are mapped to new features (e.g., `cut_Ideal=1.0`) with value 1.0"],"metadata":{}},{"cell_type":"code","source":["def xty_map(row):\n  row = row.asDict()\n  for i in features:\n    # If the features is not categorical (e.g., carat) then:\n    #    the features name is just the name of the feature and \n    #    the value is just the value\n    # otherwise the feature is categorical and we implement a one-hot-encoding\n    #    the feature name is the name + \"_\" + value (e.g., cut_Ideal)\n    #    the feature value is 1.0\n    (ki, vi) = (i, row[i]) if i not in categorical else (i+\"_\"+row[i], 1.0)\n    yield (ki, vi * row[label])"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["def xtx_map(row):\n  row = row.asDict()\n  # Iterating over features = ['cut', 'carat', ...]\n  # we are computing the outer product over each feature value:\n  for i in features:\n    (ki, vi) = (i, row[i]) if i not in categorical else (i+\"_\"+row[i], 1.0)\n    for j in features:\n      (kj, vj) = (j, row[j]) if j not in categorical else (j+\"_\"+row[j], 1.0)\n      yield ((ki,kj), vi * vj)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Debugging\n\nBecause debugging in Spark can be difficult we will want to test these functions outside of Spark first:"],"metadata":{}},{"cell_type":"code","source":["row = data.take(1)[0]\nrow"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["[a for a in xtx_map(row)]"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["[a for a in xty_map(row)]"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["## FlatMap and Reduce\n\nHere we use the [`flatMap`](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.flatMap) function which is nearly identical to the [`map`](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.map) function except that it may output multiple values which are all concatenated to the output.\n\nThe following lines will trigger computation across the data center by shipping our `xtx_map` and `xty_map` functions to remote workers wher ethe data is stored."],"metadata":{}},{"cell_type":"code","source":["xtx_data = ( data.rdd\n  .flatMap(xtx_map)\n  .reduceByKey(lambda a, b: a + b)\n  .collect()\n)\n\nxty_data = ( data.rdd\n  .flatMap(xty_map)\n  .reduceByKey(lambda a, b: a + b)\n  .collect()\n)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["In the following block of code we compute the index in the feature vector for each of the string features."],"metadata":{}},{"cell_type":"code","source":["index = dict(zip([r[0] for r in xty_data], range(len(xty_data))))\np = len(index)\nprint(\"Dimensions\", p)\nindex"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Using the index which translates each feature name to a particular location in an `x` vector we then fill out the entries of the local `XTY` and `XTX` matrices.  Notice that this is done entirely in this python notebook and not by calling Spark.  For very high dimensional problems this could be an issue."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nXTY = np.zeros((p,1))\nfor (k,v) in xty_data:\n  XTY[index[k]] = v\n\nXTX = np.zeros((p,p))\nfor ((k1,k2),v) in xtx_data:\n  XTX[index[k1], index[k2]] = v"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["To compute the optimal \\\\(\\hat{\\theta}\\\\) value we need to solve the \\\\(p \\times p\\\\) system of linear equations.  This is done by calling the linear algebra library in numpy.  This computation is running locally inside of this notebook."],"metadata":{}},{"cell_type":"code","source":["theta = np.linalg.solve(XTX, XTY)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["# Rendering Predictions\n\nIn the following block of code we define a new **user defined function** to run on the PySpark dataframe and render predictions.  We first demonstrate how this function can be applied to the raw:"],"metadata":{}},{"cell_type":"code","source":["def predict(row):\n  row = row.asDict()\n  pred = 0.0\n  for i in features:\n    (ki, vi) = (i, row[i]) if i not in categorical else (i+\"_\"+row[i], 1.0)\n    pred += vi * theta[index[ki],0]\n  return float(pred)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["The following function computes the root mean squared error in the predicted price by running a sequence of map functions which run across the datacenter."],"metadata":{}},{"cell_type":"code","source":["rmse = np.sqrt(data.rdd\n   .map(lambda row: (row[label], predict(row)))\n   .map(lambda (y,yhat): (yhat - y) * (yhat - y) )\n   .mean()\n )\nprint(\"RMSE:\", rmse)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Ideally we would like to add the predictions directly into our dataframe.  The following block of code creates a **user defined function** (UDF) and then uses the UDF to add an additional column to the Dataframe."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf, struct\nfrom pyspark.sql.types import FloatType\n\n# A UDF must have a defined return type\npredict_udf = udf(predict, FloatType())\n\n# the Struct of features states which columns in the dataframe are given to the UDF\ndata = data.withColumn(\"pred\", predict_udf(struct(features)))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["from pyspark.sql.functions import mean\ndata = data.withColumn(\"resid\", data['pred'] - data['price'])\nnp.sqrt(data.select(mean(data['resid'] * data['resid'])).collect()[0][0])"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["display(data.sample(False, 0.1).select('resid'))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["display(data.sample(False, 0.01))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from pyspark.sql.functions import lit\navg_price = data.select(mean(data['price'])).collect()[0][0]\ndata = data.withColumn(\"mean_resid\", data['pred'] - lit(avg_price))\nnp.sqrt(data.select(mean(data['mean_resid'] * data['mean_resid'])).collect()[0][0])"],"metadata":{},"outputs":[],"execution_count":47}],"metadata":{"name":"Linear Regression","notebookId":2580593937400002},"nbformat":4,"nbformat_minor":0}
