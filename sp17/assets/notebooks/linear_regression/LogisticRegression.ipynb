{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotly plotting support\n",
    "import plotly.plotly as py\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode()\n",
    "\n",
    "# import cufflinks as cf\n",
    "# cf.go_offline() # required to use plotly offline (no account required).\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Make the notebook deterministic \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This is the notebook accompanies the lecture on Logistic Regression.\n",
    "\n",
    "Notebook created by [Joseph E. Gonzalez](https://eecs.berkeley.edu/~jegonzal) for DS100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data\n",
    "\n",
    "For this lecture we will use the famous Wisconsin Breast Cancer Dataset which we can obtain from [scikit learn](http://scikit-learn.org/stable/datasets/index.html#breast-cancer-wisconsin-diagnostic-database).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "data = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "# Target data_dict['target'] = 0 is malignant 1 is benign\n",
    "data['malignant'] = (data_dict['target'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension',\n",
       "       'malignant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/901.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = go.Scatter(x=data['mean radius'], y = 1.*data['malignant'], mode=\"markers\")\n",
    "py.iplot([points], filename=\"lr-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a clear example of over-plotting.  We can improve the above plot by jittering the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/903.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jitter_y = data['malignant'] + 0.1*np.random.rand(len(data['malignant'])) -0.05\n",
    "points = go.Scatter(x=data['mean radius'], y = jitter_y, mode=\"markers\", marker=dict(opacity=0.5))\n",
    "py.iplot([points], filename=\"lr-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a picture of the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/905.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(ff.create_distplot([\n",
    "    data[data['malignant']]['mean radius'],\n",
    "    data[~data['malignant']]['mean radius']\n",
    "], group_labels=['malignant', 'benign'],  bin_size=0.5), filename=\"lr-03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Looking at the above histograms could you describe a rule to predict whether or a cell is malignant?\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Regression\n",
    "\n",
    "_\"I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail.\"_ -  Abraham Maslow The Psychology of Science\n",
    "\n",
    "**Goal:** We would like to predict whether the tumor is malignant from the size of the tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always split your data into training and test groups.  \n",
    "\n",
    "## Preparing the data Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size:  426\n",
      "Test Data Size:  143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_tr, data_te = train_test_split(data, test_size=0.25, random_state=42)\n",
    "print(\"Training Data Size: \", len(data_tr))\n",
    "print(\"Test Data Size: \", len(data_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define $X$ and $Y$ as variables containing the training features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([data_tr['mean radius']]).T\n",
    "Y = data_tr['malignant'].values.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a least squares regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.linear_model as linear_model\n",
    "\n",
    "least_squares_model = linear_model.LinearRegression()\n",
    "least_squares_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is our fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/907.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jitter_y = Y + 0.1*np.random.rand(len(Y)) - 0.05\n",
    "points = go.Scatter(name=\"Jittered Data\", \n",
    "                    x=np.squeeze(X), y = jitter_y, \n",
    "                    mode=\"markers\", marker=dict(opacity=0.5))\n",
    "X_plt = np.linspace(np.min(X), np.max(X), 10)\n",
    "model_line = go.Scatter(name=\"Least Squares\",\n",
    "    x=X_plt, y=least_squares_model.predict(np.array([X_plt]).T), \n",
    "    mode=\"lines\", line=dict(color=\"orange\"))\n",
    "py.iplot([points, model_line], filename=\"lr-04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "\n",
    "1. Are we happy with the fit?\n",
    "2. What is the meaning of predictions that are neither 0 or 1?\n",
    "3. Could we use this to make a decision?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Root Mean Squared Error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.338350570944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "print(\"Training RMSE:\", np.sqrt(mse(Y, least_squares_model.predict(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Maybe we should also look at the cross validated error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Validation Error:    0.296672238481\n",
      "Median Validation Error: 0.337995623696\n",
      "Max Validation Error:    0.395474238206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "rmse_scores = np.sqrt(\n",
    "    cross_val_score(linear_model.LinearRegression(), X, Y, \n",
    "                    scoring=make_scorer(mse), cv=5))\n",
    "\n",
    "print(\"Min Validation Error:   \", np.min(rmse_scores))\n",
    "print(\"Median Validation Error:\", np.median(rmse_scores))\n",
    "print(\"Max Validation Error:   \", np.max(rmse_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "1. Are we satisfied with this error value?\n",
    "1. Is this the right measure of error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "# Classification Error\n",
    "\n",
    "This is a classification problem so we probably want to measure how often we predict the correct value.  This is sometimes called the zero-one loss (or error):\n",
    "\n",
    "$$ \\large\n",
    "\\textbf{ZeroOneLoss} = \\frac{1}{n} \\sum_{i=1}^n \\textbf{I}\\left[ y_i == f_\\theta(x) \\right]\n",
    "$$\n",
    "\n",
    "However to use the classification error we need to define a decision rule that maps $f_\\theta(x)$ to the $\\{0,1\\}$ classification values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "# Simple Decision Rule\n",
    "\n",
    "Suppose we instituted the following simple decision rule:\n",
    "<big>\n",
    "<center>\n",
    "` If ` $f_\\theta(x) > 0.5$ `predict 1 else predict 0 `\n",
    "</center>\n",
    "</big>\n",
    "\n",
    "This simple **decision rule** is deciding that a tumor is malignant if our model predicts a values above 0.5 (closer to 1 than zero).\n",
    "\n",
    "In the following we plot the implication of these decisions on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/909.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jitter_y = Y + 0.1*np.random.rand(len(Y)) - 0.05\n",
    "ind_mal = least_squares_model.predict(X) > 0.5\n",
    "\n",
    "mal_points = go.Scatter(name=\"Classified as Malignant\", \n",
    "                    x=np.squeeze(X[ind_mal]), y = jitter_y[ind_mal], \n",
    "                    mode=\"markers\", marker=dict(opacity=0.5, color=\"red\"))\n",
    "ben_points = go.Scatter(name=\"Classified as Benign\", \n",
    "                    x=np.squeeze(X[~ind_mal]), y = jitter_y[~ind_mal], \n",
    "                    mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\"))\n",
    "dec_boundary = (0.5 - least_squares_model.intercept_)/least_squares_model.coef_[0]\n",
    "dec_line = go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[-0.5,1.5], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\"))\n",
    "py.iplot([mal_points, ben_points, model_line,dec_line], filename=\"lr-05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute `ZeroOneLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fraction incorrect: 0.133802816901\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "print(\"Training Fraction incorrect:\", \n",
    "      zero_one_loss(Y, least_squares_model.predict(X) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n",
    "\n",
    "1. Are we happy with this error level?\n",
    "1. What error would we get if we just guessed the label?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guessing the Majority Class\n",
    "\n",
    "This is the simplest baseline we could imagine and one you should always compare against.  Let's start by asking what is the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of Malignant Samples: 0.370892018779\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of Malignant Samples:\", np.mean(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore if we guess the majority class **benign** we would get what accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can figure this out from the above number\n",
    "# print(\"Guess Majority:\",  zero_one_loss(Y, np.zeros(len(Y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is standard example of a common problem in classification (and perhaps modern society): **class imbalance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance\n",
    "\n",
    "\n",
    "Class imbalance is when a disproportionate fraction of the samples are in one class (in this case benign).  In extreme cases (e.g., fraud detection) only tiny fraction of the training data may contain examples in particular class.  In these settings we can achieve very high-accuracies by always predicting the frequent class without learning a good classifier for the rare classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/>\n",
    "\n",
    "# Addressing Class Imbalance\n",
    "\n",
    "There are many techniques for managing class imbalance here are a few:\n",
    "\n",
    "1. Re-sample data to reduce or eliminate the class imbalance.\n",
    "2. Try learning algorithm that are a little more robust to class imbalance (e.g., decisions trees).\n",
    "\n",
    "In this example the class imbalance is not that extreme so we will continue without re-sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation of Zero-One Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Validation Error:    0.0845070422535\n",
      "Median Validation Error: 0.161971830986\n",
      "Max Validation Error:    0.176056338028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(3,shuffle=True, random_state=42)\n",
    "linreg_errors = []\n",
    "models = []\n",
    "for tr_ind, te_ind in kfold.split(X):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X[tr_ind,], Y[tr_ind])\n",
    "    models.append(model)\n",
    "    linreg_errors.append(zero_one_loss(Y[te_ind], model.predict(X[te_ind,]) > 0.5))\n",
    "    \n",
    "print(\"Min Validation Error:   \", np.min(linreg_errors))\n",
    "print(\"Median Validation Error:\", np.median(linreg_errors))\n",
    "print(\"Max Validation Error:   \", np.max(linreg_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty broad range?  \n",
    "\n",
    "Not very robust ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dec_lines = [\n",
    "#     go.Scatter(name=\"Decision Boundary\", \n",
    "#                x = [(0.5 - m.intercept_)/m.coef_[0]]*2, \n",
    "#                y=[-0.5,1.5], mode=\"lines\",\n",
    "#                line=dict(dash=\"dot\"))\n",
    "#     for m in models]\n",
    "\n",
    "# X_plt = np.linspace(np.min(X), np.max(X), 10)\n",
    "# model_lines = [\n",
    "#     go.Scatter(name=\"Least Squares \" + str(zero_one_loss(Y, m.predict(X) > 0.5)),\n",
    "#                x=X_plt, y=m.predict(np.array([X_plt]).T), \n",
    "#                mode=\"lines\")\n",
    "#     for m in models]\n",
    "# py.iplot([points] + model_lines + dec_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Can we think of the line as a _\"probability\"_?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "Not really.  Probabilities are constrained between 0 and 1.   How could we learn a model that captures this probabilistic interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "# Could we just truncate the line?\n",
    "\n",
    "Maybe we can define the probability as:\n",
    "\n",
    "$$ \\large\n",
    "p_i = \\min\\left(\\max \\left( x^T \\theta , 0 \\right), 1\\right)\n",
    "$$\n",
    "\n",
    "this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bound01(z):\n",
    "    u = np.where(z > 1, 1, z)\n",
    "    return np.where(u < 0, 0, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/911.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_plt = np.linspace(np.min(X), np.max(X), 100)\n",
    "p_line = go.Scatter(name=\"Truncated Least Squares\",\n",
    "    x=X_plt, y=bound01(least_squares_model.predict(np.array([X_plt]).T)), \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=8))\n",
    "py.iplot([mal_points, ben_points, model_line, p_line, dec_line], filename=\"lr-06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Satisfied Yet?\n",
    "\n",
    "So far least squares regression seems pretty reasonable and we can \"force\" the model values to be bounded between 0 and 1 (a must for probabilities).  \n",
    "\n",
    "**Can we interpret the truncated values as probabilities** perhaps but it would depend on how the model is estimated (more on this soon).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "## Issue with Extreme Points \n",
    "\n",
    "It seems like large tumor sizes are indicative of malignant tumors.  Suppose we measured a very large tumor at 100mm that is also malignant.  What would this do to our model?\n",
    "\n",
    "\n",
    "Let's add an extra data point and see what happens:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ex = np.vstack([X, [100]])\n",
    "Y_ex = np.hstack([Y, 1.])\n",
    "least_squares_model_ex = linear_model.LinearRegression()\n",
    "least_squares_model_ex.fit(X_ex, Y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/913.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_plt = np.linspace(np.min(X), np.max(X), 100)\n",
    "\n",
    "extreme_point = go.Scatter(\n",
    "    name=\"Extreme Point\", x=[100], y=[1], mode=\"markers\", \n",
    "    marker=dict(color=\"black\", size=10))\n",
    "model_line.line.color = \"gray\"\n",
    "model_line_ex = go.Scatter(name=\"New Least Squares\",\n",
    "    x=X_plt, y=least_squares_model_ex.predict(np.array([X_plt]).T), \n",
    "    mode=\"lines\", line=dict(color=\"orange\"))\n",
    "\n",
    "dec_line.line.color = \"gray\"\n",
    "\n",
    "dec_boundary_ex = (0.5 - least_squares_model_ex.intercept_)/least_squares_model_ex.coef_[0]\n",
    "dec_line_ex = go.Scatter(\n",
    "    name=\"Decision Boundary\", \n",
    "    x = [dec_boundary_ex, dec_boundary_ex], y=[-0.5,1.5], mode=\"lines\",\n",
    "    line=dict(color=\"black\", dash=\"dash\"))\n",
    "\n",
    "\n",
    "\n",
    "py.iplot([mal_points, ben_points,model_line, model_line_ex, dec_line, dec_line_ex], filename=\"lr-07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 0.133489461358\n",
      "After: 0.182669789227\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", \n",
    "      zero_one_loss(Y_ex, least_squares_model.predict(X_ex) > 0.5))\n",
    "print(\"After:\", \n",
    "      zero_one_loss(Y_ex, least_squares_model_ex.predict(X_ex) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The error got _worse_ upon observing an extreme point that agrees with our trend! **\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Return to Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "# Empirical Probability \n",
    "\n",
    "What does the empirical probability of being malignant for different sizes look like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_slices = np.linspace(8, 24, 10)\n",
    "P_emp = np.array([\n",
    "    np.mean(Y[np.squeeze(np.abs(X - c)) < 2.0]) for c in X_slices\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/913.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_emp_line = go.Scatter(name=\"Empirical Probabilities\",\n",
    "    x=X_slices, y=P_emp, \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=8))\n",
    "py.iplot([mal_points, ben_points, model_line, p_emp_line, dec_line], filename=\"lr-07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Activation Function\n",
    "\n",
    "The **logistic function** (sometimes called the sigmoid function) has the above shape and is commonly used to transform a real number into a value that can be used to encode a probability.  The logistic function has some interesting mathematical properties but let's start with it's form:\n",
    "\n",
    "$$\\large\n",
    "\\sigma(t) = \\frac{1}{1 + e^{-t}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1. / (1. + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/919.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.linspace(-5,5,50)\n",
    "sigmoid_line = go.Scatter(name=\"Logistic Function\",\n",
    "    x=t, y=sigmoid(t), mode=\"lines\")\n",
    "py.iplot([sigmoid_line], filename=\"lr-08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return to slides\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Recall the algorithm consists of iteratively computing:\n",
    "\n",
    "$$\\large\n",
    "\\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\rho(t)\n",
    "\\nabla_\\theta \\left(- \\log\\mathcal{L}(\\theta) \\right) \n",
    "$$\n",
    "\n",
    "where $\\rho(t)$ (sometimes called the learning rate) is typically:\n",
    "\n",
    "$$\n",
    "\\large \\rho(t) = \\frac{1}{t}\n",
    "$$\n",
    "\n",
    "and for logistic regression the gradient of the negative log-likelihood is given by:\n",
    "\n",
    "$$\\large\n",
    "\\nabla_\\theta \\left(- \\log\\mathcal{L}(\\theta) \\right)  = \\large \\sum_{i=1}^n \n",
    " \\left(\\sigma\\left(x_i^T \\theta\\right) - y_i \\right) x_i\n",
    "$$\n",
    "\n",
    "See the lecture slides for a derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, theta0, gradient_function, max_iter = 1000000,  \n",
    "                     epsilon=0.0001):\n",
    "    theta = theta0\n",
    "    for t in range(1, max_iter):\n",
    "        rho = 1./t\n",
    "        grad = gradient_function(theta, X, Y)\n",
    "        theta = theta - rho * grad\n",
    "        # Track Convergence\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            return theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence: When to stop iterating\n",
    "\n",
    "The gradient descent algorithm is typically run until one of the following conditions is met:\n",
    "\n",
    "1. $\\theta$ stops changing (by more than epsilon)\n",
    "1. a maximum number of iterations are run (i.e., user gets impatient)\n",
    "1. the gradient of the objective is zero (this is always a stopping condition, why?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining $\\nabla_\\theta \\left(- \\log\\mathcal{L}(\\theta) \\right)$\n",
    "\n",
    "In the following we implement the gradient function for a dataset X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_gradL(theta, X, Y):\n",
    "    return -(X.T @ (Y - sigmoid(X @ theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we standardize this dataset for numerical stability.  Note that due to the exponents extreme values can be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "standardizer.fit(X)\n",
    "Xst = standardizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add a constant (bias or offset) term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Phi = np.hstack([Xst,np.ones([len(Xst),1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the batch gradient descent solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.30903971, -0.63546791])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_batch = gradient_descent(Phi, Y, np.zeros(2), neg_gradL)\n",
    "theta_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the solution\n",
    "\n",
    "In the following we plot the logistic regression probability model and the corresponding decision boundary.  To compute the decision boundary recall that from the model definition that $\\textbf{P}(Y = 1 | X) = \\sigma(x^T \\theta)$ and therefore the probability that $\\textbf{P}(Y = 1 | X) > 0.5$ occurs when $\\sigma(x^T \\theta) > 0.5$ and therefore $x^T \\theta > \\sigma^{-1}(0.5) = 0$.  Note however that we standardized the features in $X$ to construct $\\Phi$ and therefore after solving for $X$ at the decision boundary we will need to apply the inverse standardization (multiply by the standard deviation and add back the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/921.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi_plt = np.hstack([\n",
    "    standardizer.transform(np.array([X_plt]).T), \n",
    "    np.ones((len(X_plt),1))])\n",
    "\n",
    "# The logistic regression decision boundary is when the underlying linear\n",
    "# model passes through zero\n",
    "lr_dec_boundary = (0.0 - theta_batch[1])/theta_batch[0]\n",
    "lr_dec_boundary = standardizer.inverse_transform([lr_dec_boundary])[0]\n",
    "lr_dec_line = go.Scatter(name=\"Logistic Reg. Decision Boundary\", \n",
    "                      x = [lr_dec_boundary,lr_dec_boundary], y=[-0.5,1.5], mode=\"lines\",\n",
    "                     line=dict(color=\"red\", dash=\"dot\"))\n",
    "\n",
    "lr_line = go.Scatter(name=\"SGD Logistic Regression\",\n",
    "    x=X_plt, y=sigmoid(Phi_plt @ theta_batch), \n",
    "    mode=\"lines\", line=dict(color=\"orange\", width=4))\n",
    "py.iplot([points, lr_line, dec_line, lr_dec_line], filename=\"lr-09\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the decision boundaries to our earlier histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/923.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt = ff.create_distplot([\n",
    "    data[data['malignant']]['mean radius'],\n",
    "    data[~data['malignant']]['mean radius']\n",
    "], group_labels=['malignant', 'benign'],  bin_size=0.5)\n",
    "plt.data.append(go.Scatter(name=\"Logistic Reg. Decision Boundary\", \n",
    "                      x = [lr_dec_boundary,lr_dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"red\", dash=\"dot\")))\n",
    "plt.data.append(go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\")))\n",
    "py.iplot(plt, filename=\"lr-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison in Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares Training Prediction Error: 0.133802816901\n",
      "Logistic Regression Prediction Error: 0.129107981221\n"
     ]
    }
   ],
   "source": [
    "print(\"Least Squares Training Prediction Error:\", \n",
    "      zero_one_loss(Y, least_squares_model.predict(X) > 0.5))\n",
    "print(\"Logistic Regression Prediction Error:\", \n",
    "      zero_one_loss(Y, sigmoid(Phi @ theta_batch) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the loss surface look like?\n",
    "\n",
    "Let's examine the loss function surface and our solution.  Recall that the **loss function** is the negative of the log-likelihood (minimize loss = maximize likelihood) and has the following form:\n",
    "$$\\large\n",
    "\\log \\mathcal{L}(\\theta) = -\\sum_{i=1}^n \\left[ y_i  \\log\\left(  \\sigma\\left(x_i^T \\theta \\right) \\right)+ \n",
    "\\left(1-y_i \\right)\\log \\left(1 - \\sigma\\left(x_i^T \\theta\\right) \\right) \\right]\n",
    "$$\n",
    "\n",
    "We can implement this in the following python expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_loss(theta, Phi, Y):\n",
    "    p = sigmoid(Phi @ theta)\n",
    "    return -np.sum((Y[:, np.newaxis] * np.log(p) + \n",
    "                    (1-Y[:, np.newaxis]) * np.log(1-p)), \n",
    "                   axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By evaluating the loss at a grid of points we can can construct the loss surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/925.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uvalues = np.linspace(2,8,70)\n",
    "vvalues = np.linspace(-4,3,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "lr_loss_values = lr_loss(thetas, Phi, Y)\n",
    "lr_loss_surface = go.Surface(name=\"Logistic Regression Loss\",\n",
    "        x=u, y=v, z=np.reshape(lr_loss_values,(len(uvalues), len(vvalues))),\n",
    "        contours=dict(z=dict(show=True, color=\"gray\", project=dict(z=True)))\n",
    "    )\n",
    "optimal_batch_lr_point = go.Scatter3d(name = \"Optimal Point\",\n",
    "        x = [theta_batch[0]], y = [theta_batch[1]], \n",
    "        z = lr_loss(np.array([theta_batch]).T, Phi, Y),\n",
    "        marker=dict(size=10, color=\"red\")\n",
    "    )\n",
    "py.iplot(go.Figure(data=[lr_loss_surface, optimal_batch_lr_point]), filename=\"lr-11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the gradient descent path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we plot the solution path taken by gradient descent.  \n",
    "\n",
    "However, to make the plot easy to visualize (solve in fewer more consistent iterations) I constrained the gradient descent procedure to take step's of at most length one.  This prevents the solution from exploding in the first few iterations. For those who are interested in optimization techniques this is a formal of **proximal** gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/927.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gd_thetas(X, Y, theta0, grad, max_iter = 1000000,  \n",
    "             epsilon=0.01):\n",
    "    theta = theta0\n",
    "    thetas = [theta0]\n",
    "    n = len(X)\n",
    "    for t in range(1, max_iter):\n",
    "        rho = 1./t\n",
    "        g = grad(theta, X, Y)\n",
    "        # To make the solution (and plot) more stable \n",
    "        # I have constrained the gradient step size to\n",
    "        # the unit ball.\n",
    "        if np.linalg.norm(g) > 1:\n",
    "            g = g / np.linalg.norm(g)\n",
    "        theta = theta - rho * g\n",
    "        thetas.append(theta)\n",
    "        # Track Convergence\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            return np.array(thetas)\n",
    "    return np.array(thetas)\n",
    "\n",
    "all_thetas = batch_gd_thetas(Phi, Y, np.array([3, 2.5]), neg_gradL)\n",
    "\n",
    "thata_points = go.Scatter(name=\"Theta Values\", x=all_thetas[:,0], y=all_thetas[:,1],\n",
    "                          mode=\"lines+markers\")\n",
    "lr_loss_contours = go.Contour(name=\"Logistic Regression Loss\",\n",
    "        x=uvalues, y=vvalues, z=np.reshape(lr_loss_values, (len(uvalues), len(vvalues))),\n",
    "        colorscale='Viridis', reversescale=True\n",
    "    )\n",
    "\n",
    "optimal_batch_lr_point = go.Scatter(name = \"Optimal Point\",\n",
    "        x = [theta_batch[0]], y = [theta_batch[1]],\n",
    "        marker=dict(size=10, color=\"red\")\n",
    "    )\n",
    "\n",
    "py.iplot(go.Figure(data=[lr_loss_contours, thata_points, optimal_batch_lr_point]), filename=\"lr-12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "In the following we implement **stochastic gradient descent (SGD)** to solve the logistic regression problem.  Afterwards we use much more efficient and robust libraries (and you should do the same).  However we implement SGD here for pedagogical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(X,Y, theta0, grad, max_iter = 5000, batch_size=50, epsilon=0.0001):\n",
    "    theta = theta0\n",
    "    n = len(X)\n",
    "    for t in range(1, max_iter):\n",
    "        rho = 1./t\n",
    "        i = t % n\n",
    "        g = n/batch_size * grad(theta, X[i:(i+batch_size),], Y[i:(i+batch_size)])\n",
    "        if np.linalg.norm(g) > 1:\n",
    "            g = g / np.linalg.norm(g)\n",
    "        theta = theta - rho * g\n",
    "        # Track Convergence\n",
    "        if (t % 10000) == 0:\n",
    "            if np.linalg.norm(grad(theta, X, Y)) < epsilon:\n",
    "                return theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the theta values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.29127697, -0.71010776])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_sgd = sgd(Phi, Y, np.array([2.5, 2.5]), neg_gradL)\n",
    "theta_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the SGD Solution Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the solution path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/931.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sgd_thetas(X,Y, theta0, grad, max_iter = 5000, batch_size=50, epsilon=0.0001):\n",
    "    theta = theta0\n",
    "    n = len(X)\n",
    "    thetas = [theta]\n",
    "    for t in range(1, max_iter):\n",
    "        rho = 1/t\n",
    "        i = t % n\n",
    "        g = n/batch_size * grad(theta, X[i:(i+batch_size),], Y[i:(i+batch_size)])\n",
    "        if np.linalg.norm(g) > 1:\n",
    "            g = g / np.linalg.norm(g)\n",
    "        theta = theta - rho * g\n",
    "        thetas.append(theta)\n",
    "    return np.array(thetas)\n",
    "\n",
    "\n",
    "all_thetas = sgd_thetas(Phi, Y, np.array([2.5, 2.5]), neg_gradL)\n",
    "\n",
    "thata_points = go.Scatter(name=\"Theta Values\", x=all_thetas[:,0], y=all_thetas[:,1],\n",
    "        mode=\"lines+markers\", showlegend=False\n",
    "    )\n",
    "lr_loss_contours = go.Contour(name=\"Logistic Regression Loss\",\n",
    "        x=uvalues, y=vvalues, z=np.reshape(lr_loss_values,(len(uvalues), len(vvalues))),\n",
    "        colorscale='Viridis', reversescale=True, \n",
    "    )\n",
    "optimal_batch_lr_point = go.Scatter(name = \"Optimal Point\",\n",
    "        x = [theta_sgd[0]], y = [theta_sgd[1]],\n",
    "        marker=dict(size=10, color=\"red\"), showlegend=False\n",
    "    )\n",
    "py.iplot(go.Figure(data=[lr_loss_contours, thata_points, optimal_batch_lr_point]), filename=\"lr-13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is going on?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/929.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt = ff.create_distplot([\n",
    "    data[data['malignant']]['mean radius'],\n",
    "    data[~data['malignant']]['mean radius']\n",
    "], group_labels=['malignant', 'benign'],  bin_size=0.5)\n",
    "plt.data.append(go.Scatter(name=\"Logistic Reg. Decision Boundary\", \n",
    "                      x = [lr_dec_boundary,lr_dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"red\", dash=\"dot\")))\n",
    "plt.data.append(go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\")))\n",
    "\n",
    "lr_sgd_dec_boundary = (0.0 - theta_sgd[1])/theta_sgd[0]\n",
    "lr_sgd_dec_boundary = standardizer.inverse_transform([lr_sgd_dec_boundary])[0]\n",
    "plt.data.append(go.Scatter(name=\"SGD Logistic Reg. Dec. Boundary\", \n",
    "                      x = [lr_sgd_dec_boundary,lr_sgd_dec_boundary], \n",
    "                           y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"blue\", dash=\"dot\")))\n",
    "py.iplot(plt, filename=\"lr-14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Return to Slides\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Separability\n",
    "\n",
    "The built in Scikit learn logistic regression models use $L^2$ regularization by default (a good idea).\n",
    "\n",
    "1. Why might regularization be necessary?\n",
    "1. What happens if the data are **separable**: there is a plane that separates one class from another?\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtoy = np.hstack([-np.random.rand(20) - 0.5, np.random.rand(20) + 0.5])\n",
    "Ytoy = np.hstack([np.zeros(20), np.ones(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.09158781e+01,  -4.86189917e-04])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_theta = gradient_descent(np.hstack([np.array([Xtoy]).T, np.ones([len(Xtoy),1])]), Ytoy, \n",
    "                             np.zeros(2), neg_gradL,\n",
    "                             epsilon=0.00001)\n",
    "toy_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the data are separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/955.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy_plt = np.hstack([\n",
    "    np.array([np.linspace(-2,2,100)]).T, \n",
    "    np.ones((100,1))])\n",
    "py.iplot([\n",
    "    go.Scatter(name = \"Zeros\", x=Xtoy[0:20], y=Ytoy[0:20] + 0.01 * np.random.randn(20), \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\")),\n",
    "    go.Scatter(name =\"Ones\", x=Xtoy[20:], y=Ytoy[20:] + 0.01 * np.random.randn(20), \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"red\")),\n",
    "    go.Scatter(name=\"SGD Logistic Regression\",\n",
    "    x=np.linspace(-2,2,100), y=sigmoid(X_toy_plt @ toy_theta), \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=4))\n",
    "], filename=\"lr-15\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the built in cross-validation support for logistic regression in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=100, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegressionCV(100)\n",
    "lr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/959.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_line = go.Scatter(name=\"Scikit-LR\",\n",
    "    x=X_plt, y=lr.predict_proba(np.array([X_plt]).T).T[1], \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=4))\n",
    "py.iplot([points, lr_line], filename=\"lr-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/957.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt = ff.create_distplot([\n",
    "    data[data['malignant']]['mean radius'],\n",
    "    data[~data['malignant']]['mean radius']\n",
    "], group_labels=['malignant', 'benign'],  bin_size=0.5)\n",
    "plt.data.append(go.Scatter(name=\"Logistic Reg. Decision Boundary\", \n",
    "                      x = [lr_dec_boundary,lr_dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"red\", dash=\"dot\")))\n",
    "plt.data.append(go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\")))\n",
    "plt.data.append(go.Scatter(name=\"SGD Logistic Reg. Dec. Boundary\", \n",
    "                      x = [lr_sgd_dec_boundary,lr_sgd_dec_boundary], \n",
    "                           y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"blue\", dash=\"dot\")))\n",
    "lr_sk_dec_boundary = (0.0 - lr.intercept_)/lr.coef_[0]\n",
    "\n",
    "plt.data.append(go.Scatter(name=\"SK Logistic Reg. Dec. Boundary\", \n",
    "                      x = [lr_sk_dec_boundary,lr_sk_dec_boundary], \n",
    "                           y=[0,0.25], mode=\"lines\",\n",
    "                     line=dict(color=\"pink\", dash=\"dot\")))\n",
    "\n",
    "py.iplot(plt, filename=\"lr-17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Prediction accuracy\n",
    "\n",
    "Comparing the prediction accuracy with least squares we see a bit of an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeastSquares Error: 0.140845070423\n",
      "LR Error: 0.129401993355\n"
     ]
    }
   ],
   "source": [
    "lr_errors = cross_val_score(linear_model.LogisticRegression(), X, Y, \n",
    "                scoring=make_scorer(zero_one_loss), cv=5)\n",
    "print(\"LeastSquares Error:\", np.mean(linreg_errors))\n",
    "print(\"LR Error:\", np.mean(lr_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the extreme point example from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares Extreme Point: 0.182669789227\n",
      "LR Extreme Point: 0.126463700234\n"
     ]
    }
   ],
   "source": [
    "lr_extreme = linear_model.LogisticRegression()\n",
    "lr_extreme.fit(X_ex, Y_ex)\n",
    "\n",
    "print(\"Least Squares Extreme Point:\", \n",
    "      zero_one_loss(Y_ex, least_squares_model_ex.predict(X_ex) > 0.5))\n",
    "print(\"LR Extreme Point:\", \n",
    "      zero_one_loss(Y_ex, lr_extreme.predict(X_ex) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Return to Slides\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Prediction & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We temporarily return to a simple synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/963.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "nk = 50\n",
    "Xmc = np.vstack([\n",
    "    np.random.randn(nk,2) + np.array([3,1]),\n",
    "    np.random.randn(nk,2) + np.array([1,3]),\n",
    "    0.5 * np.random.randn(nk,2) ,\n",
    "    0.5 * np.random.randn(nk,2) + np.array([3,3])\n",
    "])\n",
    "Ymc = np.hstack([np.zeros(nk), np.ones(nk), 2*np.ones(nk),2*np.ones(nk)])\n",
    "\n",
    "blue_points = go.Scatter(name = \"A\", x=Xmc[Ymc==0,0], y=Xmc[Ymc==0,1], \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\"))\n",
    "red_points  = go.Scatter(name = \"B\", x=Xmc[Ymc==1,0], y=Xmc[Ymc==1,1], \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"red\"))\n",
    "orange_points = go.Scatter(name = \"C\", x=Xmc[Ymc==2,0], y=Xmc[Ymc==2,1], \n",
    "               mode=\"markers\", marker=dict(opacity=0.5, color=\"orange\"))\n",
    "\n",
    "\n",
    "\n",
    "py.iplot([blue_points, red_points, orange_points], filename=\"lr-18\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Max Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a soft-max classifier.  Currently scikit learn requires that we use an LBFGS solver for soft-max.  L-BFGS is a quasi second order method that uses the both the gradient and a diagonal approximation of the second derivative (the Hessian) to solve for the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\n",
    "lr.fit(Xmc,Ymc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Decision Surface\n",
    "\n",
    "In the following we plot the corresponding decision surface generated by the basic multiclass logistic regression model.  Notice that the limitations of the linear decision surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/965.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uvalues = np.linspace(-4,7,70)\n",
    "vvalues = np.linspace(-3,7,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "coords = np.vstack((u.flatten(),v.flatten())).T\n",
    "label = lr.predict(coords)\n",
    "label_contour = go.Contour(\n",
    "    name=\"Label ContourSurace\",\n",
    "    x = uvalues, y=vvalues, z=np.reshape(label,(len(uvalues), len(vvalues))),\n",
    "    colorscale=[[0.0, 'rgb(100,100,100)'], [0.5, 'rgb(150,150,150)'], [1.0, 'rgb(200,200,200)']]\n",
    ")\n",
    "py.iplot(go.Figure(data=[label_contour, blue_points, red_points, orange_points]), \n",
    "         filename=\"lr-19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering with Gaussian RBFs\n",
    "\n",
    "To capture the non-linear structure we randomly sample 20 data points and create an RBF feature centered at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussian_rbf(us, lam=1):\n",
    "    return lambda x: np.array([np.exp(-np.linalg.norm(x - u)**2 / lam**2) for u in us])\n",
    "\n",
    "num_basis = 20\n",
    "np.random.seed(42)\n",
    "rbf_features = gaussian_rbf(Xmc[np.random.choice(range(len(Xmc)), num_basis, replace=False),:])\n",
    "Phimc = np.array([rbf_features(x) for x in Xmc])\n",
    "\n",
    "lr_rbf = linear_model.LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\n",
    "lr_rbf.fit(Phimc,Ymc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again fitting the logistic regression model we get a non-linear decision surface in our original space but a linear decision surface in a higher dimensional transformed space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/967.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uvalues = np.linspace(-4,7,70)\n",
    "vvalues = np.linspace(-3,7,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "coords = np.array([rbf_features(x) for x in np.vstack((u.flatten(),v.flatten())).T])\n",
    "label = lr_rbf.predict(coords)\n",
    "label_contour = go.Contour(\n",
    "    name=\"Label ContourSurace\",\n",
    "    x = uvalues, y=vvalues, z=np.reshape(label,(len(uvalues), len(vvalues))),\n",
    "    colorscale=[[0.0, 'rgb(100,100,100)'], [0.5, 'rgb(150,150,150)'], [1.0, 'rgb(200,200,200)']]\n",
    ")\n",
    "py.iplot(go.Figure(data=[label_contour, blue_points, red_points, orange_points]), \n",
    "         filename=\"lr-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/961.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uvalues = np.linspace(-4,7,70)\n",
    "vvalues = np.linspace(-3,7,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "coords = np.array([rbf_features(x) for x in np.vstack((u.flatten(),v.flatten())).T])\n",
    "prb = lr_rbf.predict_proba(coords)\n",
    "\n",
    "surfaces = [\n",
    "    go.Surface(name=str(i),\n",
    "        x=u, y=v, z=np.reshape(prb[:,i],(len(uvalues), len(vvalues))), \n",
    "               showscale = False\n",
    "    )\n",
    "    for i in range(3)\n",
    "]\n",
    "\n",
    "py.iplot(go.Figure(data=surfaces), filename=\"lr-21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds100]",
   "language": "python",
   "name": "conda-env-ds100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
