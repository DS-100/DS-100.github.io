{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Plotly plotting support\n",
    "import plotly.plotly as py\n",
    "\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode()\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Make the notebook deterministic \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In this notebook we will explore a key part of data science: _**feature engineering** the process of transforming the representation of model inputs to enable better model approximation._  Feature engineering enables you to:\n",
    "\n",
    "1. **encode** non-numeric features to be used as inputs to common numeric models\n",
    "1. capture **domain knowledge** (e.g., the perceived loudness or sound is the log of the intensity)\n",
    "1. **transform** complex relationships into simple linear relationships\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping from Domain to Range\n",
    "\n",
    "In the supervised learning setting were are given $(X,Y)$ paris with the goal of learning the mapping from $X$ to $Y$. For example, given pairs of square footage and price we want to learn a function that captures (or at least approximates) the relationship between square feet and price.  Our functional approximation is some form of typically parametric mapping from some **domain** to some **range**:\n",
    "\n",
    "<img src=\"domain_range.png\" width=\"400px\">\n",
    "\n",
    "In this class we will focus on **Multiple Regression** in which we consider mappings from potentially high-dimensional input spaces onto the real line (i.e., $y \\in \\mathbb{R}$):\n",
    "\n",
    "<img src=\"domain_real_range.png\" width=\"400px\">\n",
    "\n",
    "It is worth noting that this is distinct from **Multivariate Regression** in which we are predicting multiple (confusing?) response values (e.g., $y \\in \\mathbb{R}^q$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Domain (Features)\n",
    "\n",
    "Suppose we are given the following table:\n",
    "\n",
    "<img src=\"input_table.png\" width=\"600px\">\n",
    "\n",
    "Our goal is to learn a function that approximates the relationship between the blue and red columns.  Let's assume the range, `\"Ratings\"`, are the real numbers (this may be a problem if ratings are between [0, 5] but more on that later).\n",
    "\n",
    "**What is the _domain_ of this function?**\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The schema of the relational model provides one possible answer:\n",
    "\n",
    "```sql\n",
    "RatingsData(uid AS INTEGER, age AS FLOAT, \n",
    "            state AS STRING, hasBought AS BOOLEAN,\n",
    "            review AS STRING, rating AS FLOAT)\n",
    "```\n",
    "\n",
    "Which would suggest that the domain is then:\n",
    "\n",
    "$$\n",
    "\\textbf{Domain} = \\mathbb{Z} \\times \\mathbb{R} \\times \\mathbb{S} \\times \\mathbb{B} \\times \\mathbb{S} \\times \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Unfortunately, the techniques we have discussed so far and most of the techniques in machine learning and statistics operate on real-valued vector inputs $x \\in \\mathbb{R}^d$ (or for the statisticians $x \\in \\mathbb{R}^p$). \n",
    "\n",
    "### Goal: \n",
    "\n",
    "<img src=\"real_domain_range.png\" width=\"400px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Moreover, many of these techniques, especially the linear models we have been studying, assume the inputs are **continuous** variables in which the relative magnitude of the feature encode information about the response variable. \n",
    "\n",
    "In the following we define several basic transformations to encode features as real numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "# Basic Feature Engineering:  _Get $\\mathbb{R}$_\n",
    "\n",
    "Our first step as feature engineers is to translate our data into a form that encodes each feature as a continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The _Uninformative_  Feature: `uid`\n",
    "\n",
    "The `uid` was likely used to join the user information (e.g., `age`, and `state`) with some `Reviews` table.  The `uid` presents several questions:\n",
    "* What is the meaning of the `uid` *number*? \n",
    "* Does the magnitude of the `uid` reveal information about the rating? \n",
    "\n",
    "There are several answers:\n",
    "\n",
    "1. Although numbers, identifiers are **typically categorical** (like strings) and as a consequence the magnitude has little meaning.  In these settings we would either **drop** or **one-hot encode** the `uid`.  We will return to feature dropping and one-hot-encoding in a moment.\n",
    "\n",
    "1. There are scenarios where the magnitude of the numerical `uid` value contains important information. When user ids are created in consecutive order, larger user ids would imply more recent users.  In these cases we might to interpret the `uid` feature as a real number. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Features\n",
    "\n",
    "While uncommon there are certain scenarios where manually dropping features might be helpful:\n",
    "\n",
    "1. when the features **does not to contain information** associated with the prediction task.  Dropping uninformative features can help to address over-fitting, an issue we will discuss in great detail soon.  \n",
    "\n",
    "1. when the feature is **not available when at prediction time.**  For example, the feature might contain information collected after the user entered a rating.  This is a common scenario in time-series analysis.\n",
    "\n",
    "However in the absence of substantial domain knowledge, we would prefer to use algorithmic techniques to help eliminate features.  We will discuss this more when we return to regularization.\n",
    "\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The _Continuous_ `age` Feature\n",
    "\n",
    "The `age` feature encodes the users age.  This is already a continuous real number so no additional feature transformations are required.  However, as we will soon see, we may introduce additional related features (e.g., indicators for various age groups or non-linear transformations).\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The _Categorical_ `state` Feature\n",
    "\n",
    "\n",
    "The `state` feature is a string encoding the category (one of the 50 states).  How do we meaningfully encode such a feature as one or more real-numbers?\n",
    "\n",
    "We could enumerate the states in alphabetical order `AL=0`, `AK=2`, ... `WY=49`.  This is a form of **dictionary encoding** which maps each category to an integer.  However, this would likely be a poor feature encoding since the magnitude provides little information about the rating.  \n",
    "\n",
    "Alternatively, we might enumerate the states based on their geographic region (e.g., lower numbers for coastal states.). While this alternative dictionary encoding may provide information there is better way to encode categorical features for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "# One-Hot Encoding\n",
    "\n",
    "<img src=\"one_hot_encoding.png\" width=\"400px\">\n",
    "\n",
    "\n",
    "\n",
    "One-Hot encoding, sometimes also called **dummy encoding** is a simple mechanism to encode categorical data as real numbers such that the magnitude of each dimension is meaningful.  Suppose a feature can take on $k$ distinct values (e.g., $k=50$ for 50 states in the United Stated).  For each distinct _possible_ value a new feature (dimension) is created.  For each record, all the new features are set to zero except the one corresponding to the value in the original feature. \n",
    "\n",
    "The following is a relatively inefficient (why?) implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoding(x, categories):\n",
    "    dictionary = dict(zip(categories, range(len(categories))))\n",
    "    enc = np.zeros(len(categories))\n",
    "    enc[dictionary[x]] = 1.0\n",
    "    return enc\n",
    "\n",
    "categories = [\"cat\", \"dog\", \"apple\"]\n",
    "one_hot_encoding(\"dog\", categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "## One-Hot Encoding in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a toy dataframe of pets including their name and kind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>kind</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goldy</td>\n",
       "      <td>Fish</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scooby</td>\n",
       "      <td>Dog</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brian</td>\n",
       "      <td>Dog</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Francine</td>\n",
       "      <td>Cat</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goldy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  kind   age\n",
       "0     Goldy  Fish   0.5\n",
       "1    Scooby   Dog   7.0\n",
       "2     Brian   Dog   3.0\n",
       "3  Francine   Cat  10.0\n",
       "4     Goldy   Dog   1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"name\": [\"Goldy\", \"Scooby\", \"Brian\", \"Francine\", \"Goldy\"],\n",
    "    \"kind\": [\"Fish\", \"Dog\", \"Dog\", \"Cat\", \"Dog\"],\n",
    "    \"age\": [0.5, 7., 3., 10., 1.]\n",
    "}, columns = [\"name\", \"kind\", \"age\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a built in function to construct one-hot encodings called **`get_dummies`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cat</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Fish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cat  Dog  Fish\n",
       "0    0    0     1\n",
       "1    0    1     0\n",
       "2    0    1     0\n",
       "3    1    0     0\n",
       "4    0    1     0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df['kind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>name_Brian</th>\n",
       "      <th>name_Francine</th>\n",
       "      <th>name_Goldy</th>\n",
       "      <th>name_Scooby</th>\n",
       "      <th>kind_Cat</th>\n",
       "      <th>kind_Dog</th>\n",
       "      <th>kind_Fish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  name_Brian  name_Francine  name_Goldy  name_Scooby  kind_Cat  \\\n",
       "0   0.5           0              0           1            0         0   \n",
       "1   7.0           0              0           0            1         0   \n",
       "2   3.0           1              0           0            0         0   \n",
       "3  10.0           0              1           0            0         1   \n",
       "4   1.0           0              0           1            0         0   \n",
       "\n",
       "   kind_Dog  kind_Fish  \n",
       "0         0          1  \n",
       "1         1          0  \n",
       "2         1          0  \n",
       "3         0          0  \n",
       "4         1          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue:** While the Pandas `pandas.get_dummies` function is very convenient and even retains meaningful column labels it has one key downside.\n",
    "\n",
    "The `get_dummies` function does not take the dictionary of possible values and so will not produce the same encoding if applied across multiple dataframes with different values. This can be a big issue when rendering predictions on a new dataset.\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a widely used machine learning package in Python and provides several implementations of feature encoders for categorical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DictVectorizer` encodes dictionaries by taking keys that map to strings and applying a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec_enc = DictVectorizer()\n",
    "vec_enc.fit(df.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.5,   0. ,   0. ,   1. ,   0. ,   0. ,   1. ,   0. ],\n",
       "       [  7. ,   0. ,   1. ,   0. ,   0. ,   0. ,   0. ,   1. ],\n",
       "       [  3. ,   0. ,   1. ,   0. ,   1. ,   0. ,   0. ,   0. ],\n",
       "       [ 10. ,   1. ,   0. ,   0. ,   0. ,   1. ,   0. ,   0. ],\n",
       "       [  1. ,   0. ,   1. ,   0. ,   0. ,   0. ,   1. ,   0. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_enc.transform(df.to_dict(orient='records')).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'kind=Cat',\n",
       " 'kind=Dog',\n",
       " 'kind=Fish',\n",
       " 'name=Brian',\n",
       " 'name=Francine',\n",
       " 'name=Goldy',\n",
       " 'name=Scooby']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_enc.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the dictionary vectorizer to new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 35.,   1.,   0.,   0.,   0.,   0.,   1.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_enc.transform([\n",
    "    {\"kind\": \"Cat\", \"name\": \"Goldy\", \"age\": 35},\n",
    "    {\"kind\": \"Bird\", \"name\": \"Fluffy\"},\n",
    "    {\"breed\": \"Chihuahua\", \"name\": \"Goldy\"},\n",
    "]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the second record `{\"kind\": \"Bird\", \"name\": \"Fluffy\"}` has invalid categories and missing fields and it's encoding is entirely zero.  Is this reasonable?\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Bonus:_ sklearn `OneHotEncoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic sklearn `OneHotEncoder` encodes a column of integers corresponding to category values.  Therefore, we first need to **dictionary encode** the string values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "dtype: int8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the \"kind\" column into a category column\n",
    "kind_codes = (\n",
    "    df['kind'].astype(\"category\", categories=[\"Cat\", \"Dog\",\"Fish\"])\n",
    "        .cat.codes # Extract the category codes\n",
    ")\n",
    "kind_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Build an instance of the encoder\n",
    "onehot = OneHotEncoder()\n",
    "\n",
    "# Construct an integer column vector from the 'kind_codes' column\n",
    "column_vec_kinds = np.array([kind_codes.values]).T\n",
    "\n",
    "# Fit the encoder (which can be resued to transform other data)\n",
    "onehot.fit(column_vec_kinds)\n",
    "\n",
    "# Transform the column vector\n",
    "onehot.transform(column_vec_kinds).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "## Key Points on One-Hot Encoding\n",
    "\n",
    "While one-hot encoding is the standard mechanism for encoding **categorical** data there are a few issues to keep in mind:\n",
    "\n",
    "1. may generate **too many** dimensions/features\n",
    "    1. sparse representations are often necessary\n",
    "    1. watch out for issues with over-fitting (more on this soon)\n",
    "\n",
    "1. all possible **values must be known in advance**\n",
    "    1. unable introduce new categories when making predictions\n",
    "    1. be sure to use the same encoding when making predictions\n",
    "\n",
    "1. **missing values** are reasonably captured by a zero in all dummy features.\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The _Boolean_ `hasBought` Feature\n",
    "\n",
    "The `hasBought` feature is a boolean (0/1) valued feature but we it can have missing values:\n",
    "\n",
    "<img src=\"input_table.png\" width=\"600px\">\n",
    "\n",
    "There are a few options for encoding `hasBought`:\n",
    "\n",
    "1. **Interpret directly as numbers.** If there were no missing values then the booleans are typically treated directly as continuous values.\n",
    "\n",
    "1. **Apply one-hot encoding.** This would create two new features `hasBought=True` and `hasBought=False`.  This is probably the most general encoding but suffers from increased complexity.\n",
    "\n",
    "1. **1/-1 Encoding.** Another common encoding for booleans with missing values is:\n",
    "\n",
    "\\begin{align}\n",
    "\\textbf{True} & \\Rightarrow 1 \\\\\n",
    "\\textbf{Null} & \\Rightarrow 0 \\\\\n",
    "\\textbf{False} & \\Rightarrow -1 \n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The _Text_ `review` Feature\n",
    "\n",
    "Encoding text as a real-valued feature is especially challenging and many of the standard transformations are **lossy**. Moreover, all of the earlier transformations (e.g., one-hot encoding and Boolean representations) preserve the information in the feature. In contrast, most of the techniques for encoding text destroy information about the word order and in many cases key parts of the grammar.  \n",
    "\n",
    "Here we will discuss two widely used representations of text:\n",
    "\n",
    "* **Bag-of-Words Encoding**: encodes text by the frequency of each word\n",
    "* **N-Gram Encoding**: encodes text by the frequency of sequences of words of length $N$\n",
    "\n",
    "Both of these encoding strategies are related to the one-hot encoding with dummy features created for every word or sequence of words and with multiple dummy features having counts greater than zero.\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag-of-Words Encoding\n",
    "\n",
    "\n",
    "The bag-of-words encoding is widely used and a standard representation for text in many of the popular text clustering algorithms.  The following is a simple illustration of the bag-of-words encoding:\n",
    "\n",
    "<img src=\"bag_of_words.png\" width=\"600px\">\n",
    "\n",
    "**Notice**\n",
    "1. **Stop words are removed.** Stop-words are words like `is` and `about` that in isolation contain very little information about the meaning of the sentence.  Here is a good list of [stop-words in many languages](https://code.google.com/archive/p/stop-words/). \n",
    "1. **Word order information is lost.**  Nonetheless the vector still suggests that the sentence is about `fun`, `machines`, and `learning`.  Thought there are many possible meanings _learning machines have fun learning_ or _learning about machines is fun learning_ ...\n",
    "1. **Capitalization and punctuation are typically removed.**  \n",
    "1. **Sparse Encoding:** is necessary to represent the bag-of-words efficiently.  There are millions of possible words (including terminology, names, and misspellings) and so instantiating a `0` for every word that is not in each record would be incredibly inefficient.  \n",
    "\n",
    "**Why is it called a bag-of-words?**  A bag is another term for a **multiset**: _an unordered \n",
    "collection which may contain multiple instances of each element._  \n",
    "\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break?\n",
    "\n",
    "When professor Gonzalez was a graduate student at Carnegie Mellon University, he and several other computer scientists created the following art piece on display at the Gates Center:\n",
    "\n",
    "<img src=\"bag_of_words_art.jpg\" width=\"300px\">\n",
    "\n",
    "**Notice**\n",
    "1. The unordered collection of words in the bag.\n",
    "1. The stop words on the floor.\n",
    "1. _The missing broom._  The original sculpture had a broom attached but the janitor got confused .... \n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The N-Gram Encoding\n",
    "\n",
    "The N-Gram encoding is a generalization of the bag-of-words encoding designed to capture limited ordering information.  Consider the following passage of text:\n",
    "\n",
    "> _The book was not well written but I did enjoy it._\n",
    "\n",
    "If we re-arrange the words we can also write:\n",
    "\n",
    "> _The book was well written but I did not enjoy it._\n",
    "\n",
    "Moreover, local word order can be important when making decisions about text.  The n-gram encoding captures local word order by defining counts over sliding windows. In the following example a bi-gram ($n=2$) encoding is constructed:\n",
    "\n",
    "<img src=\"ngram.png\" width=\"800px\">\n",
    "\n",
    "The above n-gram would be encoded in the sparse vector:\n",
    "\n",
    "<img src=\"ngram_vector.png\" width=\"300px\">\n",
    "\n",
    "Notice that the n-gram captures key pieces of sentiment information: `\"well written\"` and `\"not enjoy\"`.  \n",
    "\n",
    "N-grams are often used for other types of sequence data beyond text. For example, n-grams can be used to encode genomic data, protein sequences, and click logs. \n",
    "\n",
    "**N-Gram Issues**\n",
    "1. The n-gram representation is hyper sparse and maintaining the dictionary of possible n-grams can be very costly.  The **hashing trick** is a popular solution to approximate the sparse n-gram encoding.  In the hashing trick each n-gram is mapped to a relatively large (e.g., 32bit) hash-id and the counts are associated with the hash index without saving the n-gram text in a dictionary.  As a consequence, multiple n-grams are treated as the same.\n",
    "1. As $N$ increase the chance of seeing the same n-grams at prediction time decreases rapidly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Bag-of-words and N-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some say the world will end in fire,',\n",
       " 'Some say in ice.',\n",
       " 'From what Ive tasted of desire',\n",
       " 'I hold with those who favor fire.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frost_text = [x for x in \"\"\"\n",
    "Some say the world will end in fire,\n",
    "Some say in ice.\n",
    "From what Ive tasted of desire\n",
    "I hold with those who favor fire.\n",
    "\"\"\".split(\"\\n\") if len(x) > 0]\n",
    "\n",
    "frost_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Construct the tokenizer with English stop words\n",
    "bow = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# fit the model to the passage\n",
    "bow.fit(frost_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words: [(0, 'desire'), (1, 'end'), (2, 'favor'), (3, 'hold'), (4, 'ice'), (5, 'ive'), (6, 'say'), (7, 'tasted'), (8, 'world')]\n"
     ]
    }
   ],
   "source": [
    "# Print the words that are kept\n",
    "print(\"\\nWords:\", \n",
    "      list(zip(range(0,len(bow.get_feature_names())),bow.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Encoding: \n",
      "\n",
      "Some say the world will end in fire,\n",
      "  (0, 1)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 8)\t1\n",
      "------------------\n",
      "Some say in ice.\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "------------------\n",
      "From what Ive tasted of desire\n",
      "  (0, 0)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "------------------\n",
      "I hold with those who favor fire.\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSentence Encoding: \\n\")\n",
    "# Print the encoding of each line\n",
    "for (s, r) in zip(frost_text, bow.transform(frost_text)):\n",
    "    print(s)\n",
    "    print(r)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the tokenizer with English stop words\n",
    "bigram = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "# fit the model to the passage\n",
    "bigram.fit(frost_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words: [(0, 'desire'), (1, 'end'), (2, 'favor'), (3, 'hold'), (4, 'hold favor'), (5, 'ice'), (6, 'ive'), (7, 'ive tasted'), (8, 'say'), (9, 'say ice'), (10, 'say world'), (11, 'tasted'), (12, 'tasted desire'), (13, 'world'), (14, 'world end')]\n"
     ]
    }
   ],
   "source": [
    "# Print the words that are kept\n",
    "print(\"\\nWords:\", \n",
    "      list(zip(range(0,len(bigram.get_feature_names())), bigram.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Encoding: \n",
      "\n",
      "Some say the world will end in fire,\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "------------------\n",
      "Some say in ice.\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "------------------\n",
      "From what Ive tasted of desire\n",
      "  (0, 0)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "------------------\n",
      "I hold with those who favor fire.\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSentence Encoding: \\n\")\n",
    "# Print the encoding of each line\n",
    "for (s, r) in zip(frost_text, bigram.transform(frost_text)):\n",
    "    print(s)\n",
    "    print(r)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Bonus:_ Term Frequency Scaling\n",
    "\n",
    "If we are encoding text in a particular domain (e.g., processing insurance claims) it is likely that there will be frequent terms (e.g., `insurance` or `claim`) that provide little information. However, because these terms occur frequently they can present challenges to some modeling techniques.  In these cases, additional scaling may be applied to transform the bag-of-word or n-gram vectors to emphasize the more informative terms. One of the most common scalings techniques is the **term frequency inverse document frequency (TF-IDF)** which emphasizes words that are unique to a particular record.  Because the notation is confusing, I have provided a pseudo code implementation.  However, you should use a more efficient sparse implementation like those provided in scikit learn.\n",
    "\n",
    "```python\n",
    "def tfidf(X):\n",
    "    \"\"\"\n",
    "    Input: X is a bag of words matrix (rows=records, cols=terms)\n",
    "    \"\"\"\n",
    "    (ndocs, nwords) = X.shape\n",
    "    tf = X / X.sum(axis=1)[:, np.newaxis]\n",
    "    idf = ndocs / (X > 0).sum(axis=0) \n",
    "    return tf * np.log(idf)\n",
    "```\n",
    "\n",
    "\n",
    "While these transformations are especially important when computing similarities between vector encodings of text.  We will not cover these transformations in DS100 but it is worth knowing that they exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Feature Encoding\n",
    "\n",
    "Most machine learning (ML) and statistics techniques operate on multivariate real-valued domains (i.e., vectors).  As a consequence, we need methods to encode non-continuous datatypes into meaningful continuous forms.  We discussed:\n",
    "\n",
    "1. **one-hot** (a.k.a. **dummy variable**) encoding transform categorical values into vectors of binary values with dimension equal to the number of possible values.\n",
    "1. **bag-of-words** and **n-gram** encoding transform text into frequency statistics for individual terms and groups of terms. \n",
    "\n",
    "We will now explore how feature transformations can be used to capture domain knowledge and encode complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Feature Transformations\n",
    "\n",
    "In addition to transforming categorical and text features to real valued representations, we can also often improve model performance through the use of additional feature transformations.  Let's start with a simple toy example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models for Non-Linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the potential for feature transformations consider the following *synthetic dataset*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               X          Y\n",
      "count  75.000000  75.000000\n",
      "mean   -0.607319  -0.455212\n",
      "std     6.120983  12.873863\n",
      "min    -9.889558 -25.028709\n",
      "25%    -6.191627 -10.113630\n",
      "50%    -1.196950  -1.648253\n",
      "75%     5.042387  10.910793\n",
      "max     9.737739  22.921518\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.889558</td>\n",
       "      <td>-7.221915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.588310</td>\n",
       "      <td>-10.111930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.312230</td>\n",
       "      <td>-15.816534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.095454</td>\n",
       "      <td>-19.059384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.070992</td>\n",
       "      <td>-22.349544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X          Y\n",
       "0 -9.889558  -7.221915\n",
       "1 -9.588310 -10.111930\n",
       "2 -9.312230 -15.816534\n",
       "3 -9.095454 -19.059384\n",
       "4 -9.070992 -22.349544"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"toy_training_data.csv\")\n",
    "print(train_data.describe())\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** As usual we would like to learn a model that predicts $Y$ given $X$.\n",
    "\n",
    "What does this relationship between $X \\rightarrow Y$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/689.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points = go.Scatter(name = \"Training Data\", \n",
    "                          x = train_data['X'], y = train_data['Y'], \n",
    "                          mode = 'markers')\n",
    "# layout = go.Layout(autosize=False, width=800, height=600)\n",
    "py.iplot(go.Figure(data=[train_points]), \n",
    "         filename=\"L19_b_p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the Data\n",
    "\n",
    "How would you describe this data?\n",
    "\n",
    "1. Is there a linear relationship between $X$ and $Y$?  \n",
    "1. Are there other patterns?\n",
    "1. How noisy is the data?\n",
    "1. Do we see obvious outliers?\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Linear Regression  (Review)\n",
    "\n",
    "For the remainder of this lecture we will focus on fitting least squares linear regression models.  Recall that **linear regression** models are functions of the form:\n",
    "\n",
    "\\begin{align}\\large\n",
    "f_\\theta(x) = x^T \\theta = \\sum_{j=1}^p \\theta_j x_j\n",
    "\\end{align}\n",
    "\n",
    "and **least squares** implies a loss function of the form:\n",
    "\n",
    "\\begin{align} \\large\n",
    "L_\\mathcal{D}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - f_\\theta(x) \\right)^2\n",
    "\\end{align}\n",
    "\n",
    "In the previous lecture, we derived the **normal equations** which define the loss minimizing $\\hat{\\theta}$ for: \n",
    "\n",
    "\\begin{align}\\large\n",
    "\\hat{\\theta} \n",
    "& \\large = \\arg\\min L_\\mathcal{D}(\\theta) \\\\\n",
    "& \\large = \\arg\\min \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - X \\theta \\right)^2 \\\\\n",
    "& \\large = \\left( X^T X \\right)^{-1} X^T Y\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with scikit-learn\n",
    "\n",
    "In this lecture we will use the [scikit-learn `linear_model` package](http://scikit-learn.org/stable/modules/linear_model.html) to compute the normal equations. This package supports a wide range of **generalized linear models**.  For those who are interested in studying machine learning, I would encourage you to skim through the descriptions of the various models in the `linear_model` package. These are the foundation of most practical applications of supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code creates an instance of the Least Squares Linear Regression model and the fits that model to the training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "# Fit the model to the data\n",
    "line_reg.fit(train_data[['X']], train_data['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** In the above code block we explicitly added a **bias** (**intercept**) term by setting `fit_intercept=True`.  Therefore we will not need to add an additional `constant` feature.\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Model\n",
    "\n",
    "To plot the model we will predict the value of $Y$ for a range of $X$ values.  I will call these **query points** `X_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_query = np.linspace(-10, 10, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the regression model to render predictions at each `X_query` point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that np.linspace returns a 1d vector therefore \n",
    "# we must transform it into a 2d column vector\n",
    "line_Yhat_query = line_reg.predict(\n",
    "    np.reshape(X_query, (len(X_query),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the residual we will also predict the $Y$ value for all the training points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_Yhat = line_reg.predict(train_data[['X']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualization code constructs a line as well as the residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/691.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the least squares regression line \n",
    "basic_line = go.Scatter(name = r\"$\\theta x$\", x=X_query.T, \n",
    "                        y=line_Yhat_query)\n",
    "\n",
    "# Definethe residual lines segments, a separate line for each \n",
    "# training point\n",
    "residual_lines = [\n",
    "    go.Scatter(x=[x,x], y=[y,yhat],\n",
    "               mode='lines', showlegend=False, \n",
    "               line=dict(color='black', width = 0.5))\n",
    "    for (x, y, yhat) in zip(train_data['X'], train_data['Y'], line_Yhat)\n",
    "]\n",
    "\n",
    "# Combine the plot elements\n",
    "py.iplot([train_points, basic_line] + residual_lines, filename=\"L19_b_p2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Fit\n",
    "\n",
    "1. Is our simple linear model a good fit?  \n",
    "1. What do we see in the above plot?\n",
    "1. Are there clear outliers?  \n",
    "1. Are there patterns to the residuals?\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Analysis\n",
    "\n",
    "To help answer these questions it can often be helpful to plot the residuals in a **residual plot**.  Residual plots plot the difference between the predicted value and the observed value in response to a particular covariate ($X$ dimension).  The residual plot can help reveal patterns in the residual that might support additional modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residuals = line_Yhat - train_data['Y'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/693.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot.ly plotting code\n",
    "py.iplot(go.Figure(\n",
    "    data = [dict(x=train_data['X'], y=residuals, mode='markers')],\n",
    "    layout = dict(title=\"Residual Plot\", xaxis=dict(title=\"X\"), \n",
    "                  yaxis=dict(title=\"Residual\"))\n",
    "), filename=\"L19_b_p3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Do we see a pattern? **\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Smoothing Model \n",
    "\n",
    "To better visualize the pattern we could apply another regression package. In the following plotting code we call a more sophisticated regression package `sklearn.kernel_ridge` to estimate a smoothed approximation to the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/695.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Use Kernelized Ridge Regression with Radial Basis Functions to \n",
    "# compute a smoothed estimator.  Later in this notebook we will \n",
    "# actually implement part of this  ...\n",
    "clf = KernelRidge(kernel='rbf', alpha=2)\n",
    "clf.fit(train_data[['X']], residuals)\n",
    "residual_smoothed = clf.predict(np.reshape(X_query, (len(X_query),1)))\n",
    "\n",
    "# Plot the residuals with with a kernel smoothing curve\n",
    "py.iplot(go.Figure(\n",
    "    data = [dict(name = \"Residuals\", x=train_data['X'], y=residuals, \n",
    "                 mode='markers'),\n",
    "            dict(name = \"Smoothed Approximation\", \n",
    "                 x=X_query, y=residual_smoothed, \n",
    "                 line=dict(dash=\"dash\"))],\n",
    "    layout = dict(title=\"Residual Plot\", xaxis=dict(title=\"X\"), \n",
    "                  yaxis=dict(title=\"Residual\"))\n",
    "), filename=\"L19_b_p4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the above plot suggests a cyclic pattern to the residuals. In higher dimensional settings or when many features are binary indicators (e.g., one-hot encoding) it will become difficult to interpret **residual plots**.  In these, cases we may instead examine the distribution of the residual to identify skew and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/697.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(ff.create_distplot([residuals], group_labels=['Residuals']), \n",
    "         filename=\"L19_b_p5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we see that there are several large outliers and there  appears to be a gap just above 0.  \n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic Non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have what appears to be a non-linear cyclic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/695.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(go.Figure(\n",
    "    data = [\n",
    "        dict(name = \"Residuals\", x=train_data['X'], y=residuals, \n",
    "             mode='markers'),\n",
    "        dict(name = \"Smoothed Approximation\", \n",
    "             x=X_query, y=residual_smoothed, \n",
    "             line=dict(dash=\"dash\"))],\n",
    "    layout = dict(title=\"Residual Plot\", xaxis=dict(title=\"X\"), \n",
    "                  yaxis=dict(title=\"Residual\"))\n",
    "), filename=\"L19_b_p4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can we fit this non-linear cyclic structure with a linear model?\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## What does it mean to be a _linear model_\n",
    "\n",
    "\n",
    "Let's return to what it means to be a linear model:\n",
    "\n",
    "$$\\large\n",
    "f_\\theta(x) = x^T \\theta = \\sum_{j=1}^p x_j \\theta_j\n",
    "$$\n",
    "\n",
    "In what sense is the above model **linear**? \n",
    "1. Linear in the features $x$? \n",
    "1. Linear in the parameters $\\theta$?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/> <br/>\n",
    "\n",
    "The answer is **yes** to all the above questions!  \n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Functions\n",
    "\n",
    "Consider the following alternative model formulation:\n",
    "\n",
    "$$\\large\n",
    "f_\\theta\\left( \\phi(x) \\right) = \\phi(x)^T \\theta = \\sum_{j=1}^{k} \\phi(x)_j \\theta_j\n",
    "$$\n",
    "\n",
    "where $\\phi_j$ is an *arbitrary function* from $x\\in \\mathbb{R}^p$ to $\\phi(x)_j \\in \\mathbb{R}$ and we define $k$ of these functions.  We often refer to these functions $\\phi_j$ as **feature functions** or **basis functions** and their design plays a critical role in both how we capture prior knowledge and our ability to fit complicated data.\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing Domain Knowledge\n",
    "\n",
    "\n",
    "Feature functions can be used to capture domain knowledge by:\n",
    "1. Introducing additional information from other sources\n",
    "1. Combining related features\n",
    "1. Encoding non-linear patterns\n",
    "\n",
    "Suppose I had data about customer purchases and I wanted to estimate their income:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_1 \n",
    "&= \\textbf{isWinter}(\\text{date}) \\\\\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_2 \n",
    "&= \\cos\\left( \\frac{\\textbf{Hour}(\\text{date})}{12} \\pi \\right)  \\\\\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_3 \n",
    "&= \\frac{\\text{amount}}{\\textbf{avg_spend}[\\textbf{ZipCode}[\\text{lat}, \\text{lon}]]} \\\\\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_4 \n",
    "&= \\exp\\left(-\\textbf{Distance}\\left((\\text{lat},\\text{lon}),  \\textbf{StoreA}\\right)\\right)^2 \\\\\n",
    "\\phi(\\text{date}, \\text{lat}, \\text{lon}, \\text{amount})_5 \n",
    "&= \\exp\\left(-\\textbf{Distance}\\left((\\text{lat},\\text{lon}),  \\textbf{StoreB}\\right)\\right)^2 \n",
    "\\end{align}\n",
    "\n",
    "**Notice:** In the above feature functions:\n",
    "1. The transformations are non-linear\n",
    "1. They pull in additional information\n",
    "1. They may encode implicit knowledge\n",
    "1. The functions $\\phi$ **do not** depend on $\\theta$\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear in $\\theta$\n",
    "As a consequence, while the model $f_\\theta\\left( \\phi(x) \\right)$ is no longer linear in $x$ it is still a **linear model** because it _is linear in $\\theta$_.  This means we can continue to use the normal equations to compute the optimal parameters.  \n",
    "\n",
    "To apply the normal equations we define the transformed feature matrix:\n",
    "\n",
    "<img src=\"phi_matrix.png\" width=\"400px\">\n",
    "\n",
    "Then substituting $\\Phi$ for $X$ we obtain the **normal equation**:\n",
    "\n",
    "$$ \\large\n",
    "\\hat{\\theta} = \\left( \\Phi^T \\Phi \\right)^{-1} \\Phi^T Y\n",
    "$$\n",
    "\n",
    "It is worth noting that the model is also linear in $\\Phi$ and that the $\\phi_j$ form a new **basis** (hence the term **basis functions**) in which the data live.  As a consequence we can think of $\\phi$ as mapping the data into a new (often higher dimensional space) in which the relationship between $y$ and $\\phi(x)$ is defined by a **hyperplane**. \n",
    "\n",
    "---\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data with $\\phi$\n",
    "\n",
    "In our toy data set we observed a cyclic pattern.  Here we construct a $\\phi$ to capture the cyclic nature of our data and visualize the corresponding hyperplane.\n",
    "\n",
    "\n",
    "In the following cell we define a function $\\phi$ that maps $x\\in \\mathbb{R}$ to the vector $[x,\\sin(x)] \\in \\mathbb{R}^2$ \n",
    "\n",
    "$$ \\large\n",
    "\\phi(x) = [x, \\sin(x)]\n",
    "$$\n",
    "\n",
    "**Why not:**\n",
    "\n",
    "$$ \\large\n",
    "\\phi(x) = [x, \\sin(\\theta_3 x + \\theta_4)]\n",
    "$$\n",
    "\n",
    "This would no longer be linear $\\theta$.  However, in practice we might want to consider a range of $\\sin$ basis:\n",
    "\n",
    "$$ \\large\n",
    "\\phi_{\\alpha,\\beta}(x) = \\sin(\\alpha x + \\beta)\n",
    "$$\n",
    "\n",
    "for different values of $\\alpha$ and $\\beta$.  The parameters $\\alpha$ and $\\beta$ are typically called **hyperparameters** because (at least in this setting) they are not set automatically through learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sin_phi(x):\n",
    "    return [x, np.sin(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the matrix $\\Phi$ by applying $\\phi$ to reach row (record) in the matrix $X$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.88955766,  0.44822588],\n",
       "       [-9.58831011,  0.16280424],\n",
       "       [-9.31222958, -0.11231092],\n",
       "       [-9.09545422, -0.32340318],\n",
       "       [-9.07099175, -0.34645201]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi = np.array([sin_phi(x) for x in train_data['X']])\n",
    "\n",
    "# Look at a few examples\n",
    "Phi[:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that in practice we might prefer a more efficient \"vectorized\" version of the above code:\n",
    "```python\n",
    "Phi = np.vstack((train_data['X'], np.sin(train_data['X']))).T\n",
    "```\n",
    "however in this notebook we will use more explicit `for` loop notation.\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a _Linear Model_ on $\\Phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again use the scikit-learn package to fit a linear model on the transformed space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin_reg = linear_model.LinearRegression(fit_intercept=False)\n",
    "sin_reg.fit(Phi, train_data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/699.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions at the transformed query points\n",
    "Phi_query = np.array([sin_phi(x) for x in X_query])\n",
    "sin_Yhat_query = sin_reg.predict(Phi_query)\n",
    "\n",
    "# plot the regression line\n",
    "sin_line = go.Scatter(name = r\"$\\theta_0 x + \\theta_1 \\sin(x)$ \", \n",
    "                       x=X_query, y=sin_Yhat_query)\n",
    "\n",
    "# Make predictions at the training points\n",
    "sin_Yhat = sin_reg.predict(Phi)\n",
    "\n",
    "# Plot the residual segments\n",
    "residual_lines = [\n",
    "    go.Scatter(x=[x,x], y=[y,yhat],\n",
    "               mode='lines', showlegend=False, \n",
    "               line=dict(color='black', width = 0.5))\n",
    "    for (x, y, yhat) in zip(train_data['X'], train_data['Y'], sin_Yhat)\n",
    "]\n",
    "\n",
    "py.iplot([train_points, sin_line, basic_line] + residual_lines, \n",
    "         filename=\"L19_b_p10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the residuals again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/701.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin_Yhat = sin_reg.predict(Phi)\n",
    "residuals = train_data['Y'] - sin_Yhat\n",
    "\n",
    "\n",
    "# Use Kernelized Ridge Regression with Radial Basis Functions to \n",
    "# compute a smoothed estimator. \n",
    "clf = KernelRidge(kernel='rbf')\n",
    "clf.fit(train_data[['X']], residuals)\n",
    "residual_smoothed = clf.predict(np.reshape(X_query, (len(X_query),1)))\n",
    "\n",
    "# Plot the residuals with with a kernel smoothing curve\n",
    "py.iplot(go.Figure(\n",
    "    data = [dict(name = \"Residuals\", \n",
    "                 x=train_data['X'], y=residuals, mode='markers'),\n",
    "            dict(name = \"Smoothed Approximation\", \n",
    "                 x=X_query, y=residual_smoothed, \n",
    "                 line=dict(dash=\"dash\"))],\n",
    "    layout = dict(title=\"Residual Plot\", \n",
    "                  xaxis=dict(title=\"X\"), yaxis=dict(title=\"Residual\"))\n",
    "), filename=\"L19_b_p11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/703.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(ff.create_distplot([residuals], group_labels=['Residuals']), \n",
    "         filename=\"L19_b_p12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** earlier that our residuals were more spread from -10 to 10 and now they have become more concentrated.  However, the outliers remain.  Is that a problem?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Linear Model in Transformed Space\n",
    "\n",
    "As discussed earlier the model we just constructed, while non-linear in $x$ is actually a linear model in $\\phi(x)$ and we can visualize that linear model's structure in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/705.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the data in higher dimensions\n",
    "phi3d = go.Scatter3d(name = \"Raw Data\",\n",
    "    x = Phi[:,0], y = Phi[:,1], z = train_data['Y'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(size=3),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Compute the predictin plane\n",
    "(u,v) = np.meshgrid(np.linspace(-10,10,5), np.linspace(-1,1,5))\n",
    "coords = np.vstack((u.flatten(),v.flatten())).T\n",
    "ycoords = coords @ sin_reg.coef_\n",
    "fit_plane = go.Surface(name = \"Fitting Hyperplane\",\n",
    "    x = np.reshape(coords[:,0], (5,5)),\n",
    "    y = np.reshape(coords[:,1], (5,5)),\n",
    "    z = np.reshape(ycoords, (5,5)),\n",
    "    opacity = 0.8, cauto = False, showscale = False,\n",
    "    colorscale = [[0, 'rgb(255,0,0)'], [1, 'rgb(255,0,0)']]\n",
    ")\n",
    "\n",
    "# Construct residual lines\n",
    "Yhat = sin_reg.predict(Phi)\n",
    "residual_lines = [\n",
    "    go.Scatter3d(x=[x[0],x[0]], y=[x[1],x[1]], z=[y, yhat],\n",
    "                 mode='lines', showlegend=False, \n",
    "                 line=dict(color='black'))\n",
    "    for (x, y, yhat) in zip(Phi, train_data['Y'], Yhat)\n",
    "]\n",
    "\n",
    "    \n",
    "# Label the axis and orient the camera\n",
    "layout = go.Layout(\n",
    "    scene=go.Scene(\n",
    "        xaxis=go.XAxis(title='X'),\n",
    "        yaxis=go.YAxis(title='sin(X)'),\n",
    "        zaxis=go.ZAxis(title='Y'),\n",
    "        aspectratio=dict(x=1.,y=1., z=1.), \n",
    "        camera=dict(eye=dict(x=-1, y=-1, z=0))\n",
    "    )\n",
    ")\n",
    "\n",
    "py.iplot(go.Figure(data=[phi3d, fit_plane] + residual_lines, layout=layout), filename=\"L19_b_p14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Loss Minimization \n",
    "\n",
    "Recall that in each stage of the process we have been minimizing the squared prediction error while increasing the sophistication of our model by introducing new features. When plotting the prediction error it is common to compute the **root mean squared error (RMSE)** which is the square-root of the average squared loss over the training data. \n",
    "\n",
    "$$ \\large\n",
    "\\textbf{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(Y_i - f_\\theta(X_i)\\right)^2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sq_loss(y, yhat):\n",
    "    return np.mean((yhat-y)**2)\n",
    "    \n",
    "const_rmse = np.sqrt(sq_loss(train_data['Y'], train_data['Y'].mean()))\n",
    "line_rmse = np.sqrt(sq_loss(train_data['Y'], line_Yhat))\n",
    "sin_rmse = np.sqrt(sq_loss(train_data['Y'], sin_Yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/707.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(go.Figure(data =[go.Bar(\n",
    "            x=[r'$\\theta $', r'$\\theta x$', \n",
    "               r'$\\theta_0 x + \\theta_1 \\sin(x)$'],\n",
    "            y=[const_rmse, line_rmse, sin_rmse]\n",
    "            )], layout = go.Layout(title=\"Loss Comparison\", \n",
    "                           yaxis=dict(title=\"RMSE\"))), \n",
    "         filename=\"L19_b_p15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the `sine` feature function were were able to reduce the prediction error. How could we improve further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Feature Functions\n",
    "\n",
    "We will now explore a range of generic feature transformations. However before we proceed it is worth contrasting two categories of feature functions and their applications.\n",
    "\n",
    "1. **Domain Specific Features:** In settings where our goal is to understand the model (e.g., identify important features that predict customer churn) we may want to construct meaningful features based on our understanding of the domain.  \n",
    "\n",
    "1. **Generic Features:** However, in other settings where our primary goals is to make accurate predictions we may instead introduce generic feature functions that enable our models to fit _and generalize_ complex relationships. \n",
    "\n",
    "----\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Polynomial Basis:\n",
    "\n",
    "The first set of generic feature functions we will consider is the polynomial basis:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi(x) = [x, x^2, x^3, \\ldots, x^k]\n",
    "\\end{align}\n",
    "\n",
    "We can define a generic python function to implement this basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poly_phi(k):\n",
    "    return lambda x: [x ** i for i in range(1, k+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simply the comparison of feature functions we define the following routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_basis(phi, desc):\n",
    "    # Apply transformation\n",
    "    Phi = np.array([phi(x) for x in train_data['X']])\n",
    "    \n",
    "    # Fit a model\n",
    "    reg_model = linear_model.LinearRegression(fit_intercept=False)\n",
    "    reg_model.fit(Phi, train_data['Y'])\n",
    "    \n",
    "    # Create plot line\n",
    "    X_test = np.linspace(-10, 10, 1000) # Fine grained test X\n",
    "    Phi_test = np.array([phi(x) for x in X_test])\n",
    "    Yhat_test = reg_model.predict(Phi_test)\n",
    "    line = go.Scatter(name = desc, x=X_test, y=Yhat_test)\n",
    "    \n",
    "    # Compute RMSE\n",
    "    Yhat = reg_model.predict(Phi)\n",
    "    rmse = np.sqrt(sq_loss(train_data['Y'], Yhat))\n",
    "    \n",
    "    # return results\n",
    "    return (line, rmse, reg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with $k=5$ degree polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/709.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(poly_line, poly_rmse, poly_reg) = (\n",
    "    evaluate_basis(poly_phi(5), \"Polynomial\")\n",
    ")\n",
    "\n",
    "py.iplot(go.Figure(data=[train_points, poly_line, sin_line, basic_line], \n",
    "                   layout = go.Layout(xaxis=dict(range=[-10,10]), \n",
    "                                      yaxis=dict(range=[-25,25]))), \n",
    "         filename=\"L19_b_p16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing to $k=15$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/711.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(poly_line, poly_rmse, poly_reg) = (\n",
    "    evaluate_basis(poly_phi(15), \"Polynomial\")\n",
    ")\n",
    "\n",
    "py.iplot(go.Figure(data=[train_points, poly_line, sin_line, basic_line], \n",
    "                   layout = go.Layout(xaxis=dict(range=[-10,10]), \n",
    "                                      yaxis=dict(range=[-25,25]))), \n",
    "         filename=\"L19_b_p17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like a pretty reasonable fit.  Returning to the RMSE on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/713.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot([go.Bar(\n",
    "            x=[r'$\\theta $', r'$\\theta x$', \n",
    "               r'$\\theta_0 x + \\theta_1 \\sin(x)$', \n",
    "               'Polynomial'],\n",
    "            y=[const_rmse, line_rmse, sin_rmse, poly_rmse]\n",
    "    )], filename=\"L19_b_p18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a slight improvement.  Perhaps we should increase to a higher degree polynomial? Why or why not?  We will return to this soon.\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Radial Basis Functions\n",
    "\n",
    "One of the more widely used generic feature functions are Gaussian radial basis functions.  These feature functions take the form:\n",
    "\n",
    "$$\n",
    "\\phi_{(\\lambda, u_1, \\ldots, u_k)}(x) = \\left[\\exp\\left( - \\frac{\\left|\\left|x-u_1\\right|\\right|_2^2}{\\lambda} \\right), \\ldots, \n",
    "\\exp\\left( - \\frac{\\left|\\left| x-u_k \\right|\\right|_2^2}{\\lambda} \\right) \\right]\n",
    "$$\n",
    "\n",
    "The **hyper-parameters** $u_1$ through $u_k$ and $\\lambda$ are not optimized with $\\theta$ but instead are set externally.  In many cases the $u_i$ may correspond to points in the training data.  The term $\\lambda$ defines the spread of the basis function and determines the \"smoothness\" of the function $f_\\theta(\\phi(x))$.\n",
    "\n",
    "The following is a plot of three radial basis function centered at 2 with different values of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_rbf(u, lam=1):\n",
    "    return lambda x: np.exp(-(x - u)**2 / lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/715.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpX = np.linspace(-2, 6,100)\n",
    "py.iplot([\n",
    "    dict(name=r\"$\\lambda=0.5$\", x=tmpX, \n",
    "         y=gaussian_rbf(2, lam=0.5)(tmpX)),\n",
    "    dict(name=r\"$\\lambda=1$\", x=tmpX, \n",
    "         y=gaussian_rbf(2, lam=1.)(tmpX)),\n",
    "    dict(name=r\"$\\lambda=2$\", x=tmpX, \n",
    "         y=gaussian_rbf(2, lam=2.)(tmpX))\n",
    "], filename=\"L19_b_p19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 RBF Functions $\\lambda=1$\n",
    "\n",
    "Here we plot 10 uniformly spaced RBF functions with $\\lambda=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/717.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rbf_phi(x):\n",
    "    return [gaussian_rbf(u, 1.)(x) for u in np.linspace(-9, 9, 10)]\n",
    "\n",
    "(rbf_line, rbf_rmse, rbf_reg) = evaluate_basis(rbf_phi, r\"RBF\")\n",
    "\n",
    "py.iplot([train_points, rbf_line, poly_line, sin_line, basic_line], filename=\"L19_b_p20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 RBF Functions $\\lambda=10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/719.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rbf_phi(x):\n",
    "    return [gaussian_rbf(u, 10.)(x) for u in np.linspace(-9, 9, 10)]\n",
    "\n",
    "\n",
    "(rbf_line, rbf_rmse, rbf_reg) = (\n",
    "    evaluate_basis(rbf_phi, r\"RBF\")\n",
    ")\n",
    "\n",
    "py.iplot([train_points, rbf_line, poly_line, sin_line, basic_line],\n",
    "         filename=\"L19_b_p21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a better fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/721.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot([go.Bar(\n",
    "            x=[r'$\\theta $', r'$\\theta x$', \n",
    "               r'$\\theta_0 x + \\theta_1 \\sin(x)$', \n",
    "               r\"Polynomial\",\n",
    "               r\"RBF\"],\n",
    "            y=[const_rmse, line_rmse, sin_rmse, poly_rmse, rbf_rmse]\n",
    "    )], filename=\"L19_b_p23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refusal to Accept Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/723.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crazy_rbf_phi(x):\n",
    "    return (\n",
    "        [gaussian_rbf(u,1.)(x) for u in np.linspace(-9, 9, 30)]\n",
    "    )\n",
    "\n",
    "(crazy_rbf_line, crazy_rbf_rmse, crazy_rbf_reg) = (\n",
    "    evaluate_basis(crazy_rbf_phi, \"RBF + Crazy\")\n",
    ")\n",
    "\n",
    "py.iplot([train_points, crazy_rbf_line, poly_line, sin_line, basic_line], \n",
    "         filename=\"L19_b_p24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/725.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bars = go.Bar(name = \"Train\",\n",
    "    x=[r'$\\theta $', r'$\\theta x$', r'$\\theta_0 x + \\theta_1 \\sin(x)$', \n",
    "       \"Polynomial\",\n",
    "       \"RBF\",\n",
    "       \"RBF + Crazy\"],\n",
    "    y=[const_rmse, line_rmse, sin_rmse, poly_rmse, rbf_rmse, crazy_rbf_rmse])\n",
    "\n",
    "py.iplot([train_bars], filename=\"L19_b_p25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Which is the best model?\n",
    "\n",
    "We started with the objective of minimizing the training loss (error).  As we increased the model sophistication by adding features we were able to fit increasingly complex functions to the data and reduce the loss.  However, is our ultimate goal to minimize training error?  \n",
    "\n",
    "Ideally we would like to minimize the error we make when making new predictions at unseen values of $X$.  One way to evaluate that error is use a **test dataset** which is distinct from the dataset used to train the model.  Fortunately, we have such a test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/727.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"toy_test_data.csv\")\n",
    "\n",
    "test_points = go.Scatter(name = \"Test Data\", x = test_data['X'], y = test_data['Y'], \n",
    "                       mode = 'markers', marker=dict(symbol=\"cross\", color=\"red\"))\n",
    "py.iplot([train_points, test_points], filename=\"L19_b_p26\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_rmse(phi, reg):\n",
    "    yhat = reg.predict(np.array([phi(x) for x in test_data['X']]))\n",
    "    return np.sqrt(sq_loss(test_data['Y'], yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/729.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bars = go.Bar(name = \"Test\",\n",
    "    x=[r'$\\theta $', r'$\\theta x$', r'$\\theta_0 x + \\theta_1 \\sin(x)$', \n",
    "       \"Polynomial\",\n",
    "       \"RBF\",\n",
    "       \"RBF + Crazy\"],\n",
    "    y=[np.sqrt(sq_loss(test_data['Y'], test_data['Y'].mean())), \n",
    "       test_rmse(lambda x: [x], line_reg),\n",
    "       test_rmse(sin_phi, sin_reg),\n",
    "       test_rmse(poly_phi(15), poly_reg),\n",
    "       test_rmse(rbf_phi, rbf_reg),\n",
    "       test_rmse(crazy_rbf_phi, crazy_rbf_reg)]\n",
    "    )\n",
    "\n",
    "py.iplot([train_bars, test_bars], filename=\"L19_b_p27\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened here?**\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "This is a very common occurrence in machine learning.  As we increased the model complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's happening: _Over-fitting_\n",
    "\n",
    "As we increase the expressiveness of our model we begin to **over-fit** to the variability in our training data.  That is we are learning patterns that do not **generalize** beyond our training dataset\n",
    "\n",
    "**Over-fitting** is a key challenge in machine learning and statistical inference.  At it's core is a fundamental trade-off between **bias** and **variance**: _the desire to explain the training data and yet be robust to variation in the training data_.\n",
    "\n",
    "We will study the **bias-variance** trade-off more in the next lecture but for now we will focus on the trade-off between under fitting and over fitting:\n",
    "\n",
    "<img src=\"under_over_fitting.png\" width=\"500px\">\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train, Test, and Validation Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage over-fitting it is essential to split your initial training data into a training and testing dataset.  \n",
    "\n",
    "<img src=\"train_test_split.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Before running cross validation split the data into train and test subsets (typically a 90-10 split). **Do not look at the test data until after selecting your final model.**\n",
    "\n",
    "With the remaining training data:\n",
    "1. Split the remaining training dataset k-different ways (as in the figure above)\n",
    "1. For each split train the model on the training fraction and then compute the error (RMSE) on the validation fraction.\n",
    "1. Average the error across each validation fraction to _estimate_ the test error.\n",
    "\n",
    "**Questions:**\n",
    "1. What is this accomplishing?\n",
    "1. What are the implication on the choice of $k$? \n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next time:\n",
    "\n",
    "We will dig more into the bias variance trade-off and introduce the concept of regularization to parametrically explore the space of model complexity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds100]",
   "language": "python",
   "name": "conda-env-ds100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
