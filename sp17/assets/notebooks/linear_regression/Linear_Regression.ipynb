{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we are going to make some synthetic data which we will use to illustrate the basic ideas of modeling design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/203.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100      # Number of records \n",
    "\n",
    "noise = 0.5  # Noise in observations (we wouldn't know this in real life)\n",
    "m = 1.5      # The true slope (we wouldn't know this in real life) \n",
    "b = 3.0      # The true intercept (we wouldn't know this in real life)\n",
    "\n",
    "# Make the data --------------------------\n",
    "X = np.random.rand(n) * 4. - 2.\n",
    "# The Y values are created using the secret model \n",
    "#      (We wouldn't have this in real-life either)\n",
    "Y = m * X + b + np.random.randn(n) * noise\n",
    "\n",
    "# Visualize the data ---------------------\n",
    "raw_data = go.Scatter(name = \"Data\", x = X, y = Y, mode = 'markers')\n",
    "py.iplot([raw_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simplest model\n",
    "\n",
    "What is the simplest model for this data?  Recall that we are trying to \"model\" the relationship between $x$ and $y$.  Let's focus on the class of **parametric** models that are functional relationships defined by a set of **parameters** $\\theta$:\n",
    "\n",
    "$$\n",
    "f_\\theta: x \\rightarrow y\n",
    "$$\n",
    "\n",
    "### The constant function:\n",
    "\n",
    "Here is a very simple model:\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\theta\n",
    "$$\n",
    "\n",
    "Notice that this model is determined by a single number $\\theta$.  How should we determine $\\theta$?  Perhaps we can guess a good value for $\\theta$?  I propose we use the mean value of $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/205.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define my model:\n",
    "def f(theta, x):\n",
    "    return theta \n",
    "\n",
    "theta_guess = np.mean(Y)\n",
    "yhat = [f(theta_guess, x) for x in X]\n",
    "\n",
    "# Visualize the data ---------------------\n",
    "theta_guess_line = go.Scatter(name=\"First Guess\", x = X, y = yhat, mode = 'lines',\n",
    "    line = dict(color = 'red')\n",
    ")\n",
    "py.iplot([raw_data, theta_guess_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this the best model?\n",
    "\n",
    "Ideally we want this model to approximate the data.  How do we know if the model approximates the data?  We can introduce a **loss function** to measure how well the model approximates the data.  A very common loss function is the squared loss which measures the squared distance between what the model predicts and the true value.  \n",
    "\n",
    "$$\n",
    "L(y, f_\\theta(x)) = \\left( y - f_\\theta(x) \\right)^2\n",
    "$$\n",
    "\n",
    "That is the loss for a single data point.  If we have many data points (and hopefully we do) then we define the loss over the entire dataset as:\n",
    "\n",
    "$$\n",
    "L_D(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, f_\\theta(x_i))\n",
    "$$\n",
    "\n",
    "Notice that in the above notation we have defined the loss $L_D(\\theta)$ as a function of only the parameters $\\theta$ and are assuming the model form $f$ and the data $D$ are fixed.  If we consider the squared loss we get:\n",
    "\n",
    "$$\n",
    "L_D(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i -  f_\\theta(x_i) \\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "<br/><br/><br/>\n",
    "We can plot the **loss function** for our simple model and our best guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/207.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sq_loss(y, pred):\n",
    "    return (y - pred)**2\n",
    "\n",
    "def loss(theta):\n",
    "    return np.mean([sq_loss(y, f(theta, x)) for (x,y) in zip(X,Y)])\n",
    "\n",
    "theta_values = np.linspace(0, 5, 100)\n",
    "loss_values = [loss(theta) for theta in theta_values]\n",
    "\n",
    "\n",
    "### Plotting code\n",
    "loss_curve = go.Scatter(name=\"Loss Function\", x = theta_values, y = loss_values, mode = 'lines')\n",
    "best_guess = go.Scatter(name=\"Best Guess\", x = theta_guess, y = loss(theta_guess), mode = 'markers',\n",
    "                       marker=dict(color=\"red\", size=20))\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        title=\"Theta Value\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Loss Value\"\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data = [loss_curve,best_guess], layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lucky guess!  Could we have used calculus to minimize the loss directly? It would look something like:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta} L_D(\\theta) \n",
    "& = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta} \\left( y_i -  f_\\theta(x_i) \\right)^2 \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i -  f_\\theta(x_i) \\right) \\frac{\\partial}{\\partial \\theta} f_\\theta(x_i) \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i -  \\theta \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where the last line comes from the assumption that our model is $f_\\theta(x) = \\theta$ and therefore $\\frac{\\partial}{\\partial \\theta} f_\\theta(x_i) = 1$.  Setting this equal to zero we get:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "0 & = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i -  \\theta \\right)  \\\\\n",
    "0 & = \\frac{2}{n} \\sum_{i=1}^n y_i -  \\theta \\frac{2 n}{n} \\\\\n",
    "\\theta \\frac{2 n}{n} & = \\frac{2}{n} \\sum_{i=1}^n y_i   \\\\\n",
    "\\theta  & = \\frac{1}{n} \\sum_{i=1}^n y_i   \n",
    "\\end{align*}\n",
    "\n",
    "Therefore the loss minimizing assignment to $\\theta$ is the average value of $y$.\n",
    "\n",
    "**Could we improve our model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You may recall from introductory math classes the equation of a line:\n",
    "\n",
    "$$\n",
    "y = m \\, x + b\n",
    "$$\n",
    "\n",
    "a line is one of the simplest models we might use to represent the relationship between an input $x$ and an output $y$.  We could write this as the model:\n",
    "\n",
    "$$\n",
    "f_{(m,b)}(x) = m \\, x + b\n",
    "$$\n",
    "\n",
    "We would call this a parametric model as it is parameterized by the parameters $m$ and $b$.  Adopting our earlier notation we can define $\\theta = (m, b)$ and rewrite our improved model:\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\theta_0 x + \\theta_1\n",
    "$$\n",
    "\n",
    "Now that we have a new model how should we select our parameters?  We would like to minimize our loss but what does it look like. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/209.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(theta, x):\n",
    "    return theta[0] * x + theta[1]\n",
    "\n",
    "def sq_loss(y, pred):\n",
    "    return (y - pred)**2\n",
    "\n",
    "def loss(theta):\n",
    "    return np.mean([sq_loss(y, theta[0] * x + theta[1] ) for (x,y) in zip(X,Y)])\n",
    "\n",
    "\n",
    "# To visualize the loss I need to generate many theta0, theta1 pairs. \n",
    "# This is done using meshgrid\n",
    "(theta0,theta1) = np.meshgrid(np.linspace(0,3,20), np.linspace(1,5,20))\n",
    "theta_values = np.vstack((theta0.flatten(), theta1.flatten())).T\n",
    "\n",
    "# Evaluating the loss for all theta values\n",
    "loss_values = [loss(theta) for theta in theta_values]\n",
    "\n",
    "# Plotting code! ------------------------\n",
    "\n",
    "# Here I make a 3d surface plot.  X, Y, and Z must all be _matrices_ \n",
    "# corresponding to the x, y, z values for each point on the x,y plane \n",
    "loss_surface = go.Surface(\n",
    "    x = theta0,\n",
    "    y = theta1,\n",
    "    z = np.reshape(loss_values, theta1.shape)\n",
    ")\n",
    "\n",
    "# Axis labels\n",
    "layout = go.Layout(\n",
    "    scene=go.Scene(\n",
    "        xaxis=go.XAxis(title='theta_0'),\n",
    "        yaxis=go.YAxis(title='theta_1'),\n",
    "        zaxis=go.ZAxis(title='loss'),\n",
    "        aspectratio=dict(x=1.,y=1., z=1.)\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data = [loss_surface], layout = layout)\n",
    "py.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with the above plot we might guess something like $\\theta_0 = 1.5$ and $\\theta_1 = 3$ (take it for a spin if you haven't already).\n",
    "\n",
    "Let's now try to derive the minimizing values for our parameters:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_0} L_D(\\theta) \n",
    "& = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta_0} \\left( y_i -  f_\\theta(x_i) \\right)^2 \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i -  f_\\theta(x_i) \\right) \\frac{\\partial}{\\partial \\theta_0} f_\\theta(x_i) \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i - \\left(\\theta_0 x_i + \\theta_1 \\right) \\right) x_i\n",
    "\\end{align*}\n",
    "\n",
    "where the last line comes from the assumption that our model is $f_\\theta(x) = \\theta_0 x + \\theta_1$ and therefore $\\frac{\\partial}{\\partial \\theta_0} f_\\theta(x) = x$.  Taking the derivative with respect to $\\theta_1$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_1} L_D(\\theta) \n",
    "& = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta_1} \\left( y_i -  f_\\theta(x_i) \\right)^2 \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i -  f_\\theta(x_i) \\right) \\frac{\\partial}{\\partial \\theta_1} f_\\theta(x_i) \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i -  \\left(\\theta_0 x_i + \\theta_1\\right) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Setting both derivatives equal to zero we get the following system of equations:\n",
    "\n",
    "\\begin{align*}\n",
    "0 & = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i -  \\theta_0 x_i - \\theta_1 \\right) x_i \\\\\n",
    "0 & = \\frac{1}{n} \\sum_{i=1}^n  2 \\left( y_i -  \\theta_0 x_i - \\theta_1 \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some algebra to make things a bit clearer:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n  x_i y_i  & =   \\theta_0  \\sum_{i=1}^n  x_i x_i + \\theta_1  \\sum_{i=1}^n  x_i \\\\\n",
    "\\sum_{i=1}^n y_i  & =   \\theta_0  \\sum_{i=1}^n  x_i + \\theta_1 n\n",
    "\\end{align*}\n",
    "\n",
    "Defining some constants:\n",
    "\n",
    "\\begin{align*}\n",
    "C_1 & = \\sum_{i=1}^n  x_i y_i  \\quad &  \\quad C_2  &=  \\sum_{i=1}^n  x_i x_i \\\\\n",
    "C_3 & = \\sum_{i=1}^n  x_i \\quad & \\quad C_4 & = \\sum_{i=1}^n y_i \n",
    "\\end{align*}\n",
    "\n",
    "We clearly have a system of linear equations:\n",
    "\n",
    "\\begin{align*}\n",
    "C_1 & =   \\theta_0  C_2 + \\theta_1  C_3 \\\\\n",
    "C_4 & =   \\theta_0  C_3 + \\theta_1 n\n",
    "\\end{align*}\n",
    "\n",
    "Which we can solve using more algebra.  However, because algebra is hard we can use symbolic algebra tools to solve these equations analytically:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAAyBAMAAABMq6pFAAAAMFBMVEX///8AAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAEImZRO/dMlQiu6vN\nZnZmcXX2AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAHAklEQVRoBdVab4hUVRQ/b2Z2Zsd1nUFRsH87\nIgWh6Kr5xawdBOtjFlIUhgomSJZDUvRB2AmxrKAdCoL+0CpBHxJCCQLZrPlUQUsu9aE/X5wSv1Tk\napKa2XbOvffcd+57d2bevmY/eGHf+93zO+d37nnvzbv3XQXAtnCYjjdee7vOY849XGN4Y50HHyub\nAY+1vQm3HZuYer19WZKWuH1EnJFxEkvP4NeJU8WKsUinN3dqY/Fv6S7xgscBDuJfmyZpidu4e80y\nTmLH+dwU9K+f0ibHqXhNGwcuOP5hJ/gA71P2amhwkaQldr0692ScxCrqNRN7E13rkYbqRZxGaspa\naqpT/DCfasttiRPaImmJ2/n77DJOYuXLJZzGKwmbdXjEabSlzKXjmo0dR+poKgzH7MYgaYnb+fvs\nMk5i5WtKyF6h3n4dHnEa0qMbalNC8S+KChp0zK1464/PCIRN0hKHHt2RjJNYR5oSxndQd6myRZ3M\n2NuVkPlXRQEcwF/EvI9gSBVjbABMZ86esBjgiOWTANagFIxJTzdTwuqW7i5/v2ydOFGXEvKXdej+\nTwCezzdhvKb75sj0PbABGEN/u1eDE2o7HEcpGJOebqaEfTpvpjHQtE6cqEsJfWo4AcCLAOWhKpwh\n4Vt2U/sSEdNrkWAMC5OVEHxBKntrNg5TMCY9bCsnJ5+YnPwa0fYy9cv5RuGqdeJEXUrIT1PoS6oE\nGAP4lLphszTdBXb9OVkJrGLjsASLY3eBvPtr+WrxUuhkEnUpQc8Iw7qEjQDX8YaIZungETN5DEPh\nhdmVYDWwBItRTzfzIKkZIYsmfErZiRN1KaF4EcMyeBdRHx5CgYxWNkdL/94CxplgdiVwHKWwGPV0\n4zdSBbvL8C8fT9SlBBjDeX0NhpL+Jcg0v0csGtN04w3+cZYlcJy6SlJPpTEl9F/HZQ5eSViOf5FE\nbgm3TonhKRhsmLiLAJaQ2wHFU3VltQemYbwOGgctfwlxaRZhDbrRjElPNVMCLDr2cRUNg/hMRxM5\nJeSOHlRxngPpt2345I5z8f3r1v3JWAR0kGavMIXQ4xKM0ya4n91tIqeEZ8ulMntEzocifbe7B86F\ngUXfb6GDNEuJFKGenpDZpfDV4aOM8cnWiZwSnoJSPfSQ6I19kaWFJAGe+eVkaPjpWj3sMGovzR4y\nhaPHDnjum5mZDrsmkVnfjVeQGdwCJc8zEMakR3MoDaUjalxnqnjKD8NoXXV7fphDaZivb8xuGvQD\nVThf5dE/yaAn5zmUBnrh4uR1lY6fT0xshwWrKoSDbXTsWVPStBbF1mNpHHcDVW+vkvZegH/gW3iV\ncI+bkqa16Fy0ga249DyhlC9C4QrsAvlinUnXxGtDj1lJq/nX1pBOORKlEx1qwZI1JBxcgeyW4DL0\n1W2aXgElrRdavZKUOs9VANbX0BJcglIL1+P5KUnPBtM3lbcp6R6UkNmgLnYkRwYfJMiqj7OtcAcU\nLtBiMF1T31T+UJLuQQl3Rj9YVLbVdANAvVTvLT4K9FXUQkOa14b6piI1bENNdeIDSXMJaaSNzh4Y\nbbCkPef0972aDxadLQP+FkpVYlPMC+qbyijndxqgTyTNJaSRNmK7YLxqYHiaN62wWmAo5L6R2NHZ\n0WRjeGZafPm/HLIWhWtRa7KAJazBBUyP0aVwm9nGM9tJyJ31zgvnxI6mK6B6TIufUcXjJtaiMZYl\nYoQ2MP10nHdWqkRn1np+9HJHM64BlqZvKt2CFqPwLNeioVUjKxElXDozHOdjJcRd0HKabt9mL0VG\nptU3lfYabOvsJ1jCz9oM33n4RCU4O5pxEUuLb6q4V0eLlfB7MZ2r5Goxj0QljO+guKUq2LOzyrT7\nTaW8kx5Ygvw3nXzvt5YbyPSBw6/Ef86JSpA7mp6dVabdbyp3EF16LEHz+/CHrUCtnMMYprfPzIRG\nRolKkDuanp1VQ7NkmrPIUKjvhaJaLYRCHTMkKkHuaHp2Vg0dZpw9EhkCXFANTqMEb7oi7JghUQn7\naExmR9Ozs2poWHV3/EGlyARNZuifhoGmG9MxQ6ISRhqoaHY0YWNsZ9XQ2ansETdz8p7MkG3iLoR7\nMTpmSFSCWn4swwHR7BvfWTV0tqLuf/JxC0+Zoa8CY+WKIAE6ZkhUgtzR9OysWjr9XbASOL/jSm7l\nYM0pwdK+DKYEs53kxImO2NH07awaGha7iYVAVygy3Afwzg+RgE4ZzPqurxmJ8Xa7zL4LvvFGzcaY\nJsNoRWUYiMwl3rRdZ1/fbfYqtTOmynBe33v+XwDttJW96+xbVP863FGjM5kmQ8D/9+J8q7N4AnZx\nraj21RK4pnTxZljyoFEbvOa+iVMkyTcWsFqK6CQhvgz92+zAbz6eRKSTT2HVikYn/v9zvgzvTpHu\nfxlEwG3H8Zq9AAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$$\\left \\{ \\theta_{0} : \\frac{C_{1} n - C_{3} C_{4}}{C_{2} n - C_{3}^{2}}, \\quad \\theta_{1} : \\frac{- C_{1} C_{3} + C_{2} C_{4}}{C_{2} n - C_{3}^{2}}\\right \\}$$"
      ],
      "text/plain": [
       "⎧    C₁⋅n - C₃⋅C₄      -C₁⋅C₃ + C₂⋅C₄⎫\n",
       "⎪θ₀: ────────────, θ₁: ──────────────⎪\n",
       "⎨              2                  2  ⎬\n",
       "⎪     C₂⋅n - C₃          C₂⋅n - C₃   ⎪\n",
       "⎩                                    ⎭"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "\n",
    "# Define the variables (symbols) that we will use in our equation\n",
    "theta0 = sympy.Symbol(\"theta0\")\n",
    "theta1 = sympy.Symbol(\"theta1\")\n",
    "c1 = sympy.Symbol(\"C1\")\n",
    "c2 = sympy.Symbol(\"C2\")\n",
    "c3 = sympy.Symbol(\"C3\")\n",
    "c4 = sympy.Symbol(\"C4\")\n",
    "n = sympy.Symbol(\"n\")\n",
    "\n",
    "# Solve the system of equations (eq = 0) for theta0 and theta1\n",
    "theta_hats_symb = sympy.solve(\n",
    "    [\n",
    "        theta0 * c2 + theta1 * c3 - c1, \n",
    "        theta0 * c3 + theta1 * n - c4 \n",
    "    ], \n",
    "    [theta0, theta1])\n",
    "\n",
    "\n",
    "# Print the answer (so pretty)\n",
    "sympy.init_printing()\n",
    "theta_hats_symb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the actual numerical values for the algebraic expressions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.44252835  2.99260477]\n"
     ]
    }
   ],
   "source": [
    "# Compute the actual numerical values for all the constants\n",
    "subs = {\n",
    "    c1: np.sum(X * Y),\n",
    "    c2: np.sum(X * X),\n",
    "    c3: np.sum(X),\n",
    "    c4: np.sum(Y),\n",
    "    n: len(X)\n",
    "}\n",
    "\n",
    "# For each theta value substitute the numerical values for the constans \n",
    "# and evaluate the expression\n",
    "theta_hat = np.array([float(theta_hats_symb[theta0].evalf(subs=subs)), \n",
    "         float(theta_hats_symb[theta1].evalf(subs=subs))])\n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now have the \"optimal\" parameters for our original model. Returning to our earlier plot lets place the solution on the loss suface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/211.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_hat_point = go.Scatter3d(\n",
    "    x = theta_hat[0],\n",
    "    y = theta_hat[1],\n",
    "    z = loss(theta_hat),\n",
    "    mode = 'markers', \n",
    "    marker = dict(color='red')\n",
    ")\n",
    "\n",
    "loss_surface.opacity = 0.9\n",
    "fig = go.Figure(data = [loss_surface, theta_hat_point], layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the line defined by these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/213.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define my model:\n",
    "yhat = [f(theta_hat, x) for x in X]\n",
    "\n",
    "# Visualize the data ---------------------\n",
    "theta_line = go.Scatter(name=\"Linear Model\", x = X, y = yhat, mode = 'lines',\n",
    "    line = dict(color = 'red')\n",
    ")\n",
    "theta_guess_line.line.color = \"gray\"\n",
    "py.iplot([raw_data, theta_guess_line, theta_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a better fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize our earlier model formulation to the general **linear model**:\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\sum_{j=1}^p \\theta_j x_j\n",
    "$$\n",
    "\n",
    "**Note:**\n",
    "1. This is the model for a **single data point**\n",
    "1. The data point is **$p$-dimensional**\n",
    "1. The subscript $j$ is indexing each of the $p$ dimensions\n",
    "\n",
    "To simplify the presentation we will use the following vector notation:\n",
    "\n",
    "$$\n",
    "f_\\theta(x) =  x^T \\theta\n",
    "$$\n",
    "\n",
    "You can see this in the following figure:\n",
    "\n",
    "<img src=\"vector_dot.png\" width=\"400px\">\n",
    "\n",
    "As we will see, shortly this is a very expressive parametric model.  In previous lectures we derived the **Least Squares** parameter values for this model.  Let's derive them again.\n",
    "\n",
    "## Least Squares Linear Regression\n",
    "\n",
    "Before we proceed we need to define some new notation to describe the entire dataset:\n",
    "\n",
    "We introduce the design (covariate) matrix $X$ and the response matrix (vector) $Y$ which encode the data:\n",
    "\n",
    "<img src=\"design_matrix.png\" width=\"400px\">\n",
    "\n",
    "**Notes:**\n",
    "1. The **rows** of $X$ correspond to records (e.g., users in our database)\n",
    "1. The **columns** of $X$ correspond to **features** (e.g., the age, income, height).\n",
    "1. **CS and Stats Terminology Issue:** In statistics we use $p$ to denote the number of columns in $X$ which corresponds to the number of *parameters* in the corresponding linear model.  In computer science we use $d$ instead of $p$ to refer to the number of columns.  The $d$ is in reference to the *dimensionality* of each record.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Minimization:\n",
    "\n",
    "We can write the loss using the matrix notation:\n",
    "\n",
    "\\begin{align}\n",
    "L_D(\\theta) \n",
    "& = \n",
    "\\frac{1}{n}\\sum_{i=1}^n \\left(Y_i -  f_\\theta(X_i)\\right)^2 \\\\\n",
    "& = \n",
    "\\frac{1}{n}\\sum_{i=1}^n \\left(Y_i -  X_i \\theta \\right)^2 \\\\\n",
    "& = \n",
    "\\frac{1}{n}\\left(Y -  X \\theta \\right)^T \\left(Y -  X \\theta \\right) \n",
    "\\end{align}\n",
    "\n",
    "Note that the last line $X_i \\theta$ is the dot product of the \n",
    "\n",
    "<img src=\"matrix_dot.png\" width=\"400px\">\n",
    "\n",
    "We can further simply the last expression by expanding the product:\n",
    "\n",
    "\\begin{align}\n",
    "L_D(\\theta) \n",
    "& = \n",
    "\\frac{1}{n}\\left(Y -  X \\theta \\right)^T \\left(Y -  X \\theta \\right) \\\\\n",
    "& =\n",
    "\\frac{1}{n} \\left( \n",
    " Y^T Y -  2 Y^T X \\theta + \\theta^T  X^T  X \\theta \n",
    "\\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Recall our goal is to compute:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta} = \\arg \\min_\\theta L(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Which we can compute by taking the **gradient** of the loss and setting it equal to zero.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta L(\\theta) \n",
    "& =\n",
    "\\frac{1}{n} \\left( \n",
    " \\nabla_\\theta Y^T Y -  \\nabla_\\theta 2 Y^T X \\theta + \\nabla_\\theta \\theta^T  X^T  X \\theta \n",
    "\\right) \\\\\n",
    "& =\n",
    "\\frac{1}{n} \\left( \n",
    " 0 -  2 X^T Y  +  2 X^T  X \\theta \n",
    "\\right) \n",
    "\\end{align}\n",
    "\n",
    "The above gradient derivation uses the following identities:\n",
    "1. $\\nabla_\\theta \\left( A \\theta  \\right) = A^T$\n",
    "1. $\\nabla_\\theta \\left( \\theta^T A \\theta \\right) = A\\theta + A^T \\theta$ and $A = X^T X$ is symmetric\n",
    "\n",
    "Setting the gradient equal to zero we get:\n",
    "\n",
    "$$\n",
    "X^T  X \\theta =  X^T Y\n",
    "$$\n",
    "\n",
    "and solving for $\\theta$ we get the famous **Normal Equation**:\n",
    "\n",
    "$$\n",
    " \\theta = \\left(X^T  X\\right)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the normal equations to our regression problem from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1904575072\n"
     ]
    }
   ],
   "source": [
    "# Because X is 1 dimensional ... we need to use scalar inversion\n",
    "theta_hat = (X.T @ X)**-1 * (X.T @ Y)\n",
    "\n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our least-squares regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/215.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = X * theta_hat\n",
    "\n",
    "# Visualize the data ---------------------\n",
    "normal_equation_line = go.Scatter(name=\"Normal Equation\", x = X, y = yhat, mode = 'lines',\n",
    "    line = dict(color = 'red')\n",
    ")\n",
    "py.iplot([raw_data, normal_equation_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuck on the origin! (again)\n",
    "\n",
    "Remember our model doesn't have **\"bias\"** term:\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\sum_{j=1}^p \\theta_j x_j\n",
    "$$\n",
    "\n",
    "However we can begin our journey into **feature engineering** by adding an extra **constant** (typically 1) dimension:\n",
    "\n",
    "<img src=\"adding_bias_term.png\" width=\"400px\">\n",
    "\n",
    "For reasons we will see later, I am going to call this a feature transformation $\\phi(X)$.  For notational convenience I will occasionally write $\\Phi = \\phi(X)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.50183952,  1.        ],\n",
       "       [ 1.80285723,  1.        ],\n",
       "       [ 0.92797577,  1.        ],\n",
       "       [ 0.39463394,  1.        ],\n",
       "       [-1.37592544,  1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phi(x):\n",
    "    return np.array([x, 1.])\n",
    "\n",
    "Phi = np.array([phi(x) for x in X]) # more efficient: Phi = np.vstack((X, np.ones(n))).T\n",
    "Phi[:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the normal equations again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.44252835  2.99260477]\n"
     ]
    }
   ],
   "source": [
    "theta_hat = np.linalg.inv(Phi.T @ Phi) @ Phi.T @ Y\n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more numerically stable and efficient way to compute $A^{-1} b$ is to use `np.linalg.solve` which computes the solution to $A \\theta = b$ rather than first computing inverse of $A$ and then multiplying by $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.44252835  2.99260477]\n"
     ]
    }
   ],
   "source": [
    "theta_hat = np.linalg.solve(Phi.T @ Phi, Phi.T @ Y)\n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the fit once more.  Notice this time  `yhat` (i.e., $\\hat{y}$) is computed using `Phi` (i.e., $\\Phi$) instead of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/217.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = Phi @ theta_hat # Phi instead of X\n",
    "\n",
    "# Visualize the data ---------------------\n",
    "normal_equation_line2 = go.Scatter(name=\"Normal Equation\", x = X, y = yhat, mode = 'lines',\n",
    "    line = dict(color = 'red')\n",
    ")\n",
    "py.iplot([raw_data, normal_equation_line2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success! **\n",
    "\n",
    "---\n",
    "\n",
    "## Moving to a Higher Dimensional Plane\n",
    "\n",
    "Something interesting just happened.  The linear model went from **one** dimension to **two** dimensions.  We added a **feature** (i.e., a dimension) to each record. In a sense we took points on the `(x,y)` plane above and moved them into 3-dimensions: `(x, 1, y)`.\n",
    "\n",
    "We can actually plot the data in 3 dimensions and see the corresponding plane of best fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jegonzal/219.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw3d = go.Scatter3d(\n",
    "    x = Phi[:,0],\n",
    "    y = Y,\n",
    "    z = Phi[:,1],\n",
    "    mode = 'markers',\n",
    "    opacity = 0.5, \n",
    "    marker = dict(size=3)\n",
    ")\n",
    "\n",
    "(u,v) = np.meshgrid(np.linspace(-3,3,20), np.linspace(-3,3,20))\n",
    "coords = np.vstack((u.flatten(),v.flatten())).T\n",
    "ycoords = coords @ theta_hat\n",
    "\n",
    "fit_plane = go.Surface(\n",
    "    x = np.reshape(coords[:,0], (20,20)),\n",
    "    y = np.reshape(ycoords, (20,20)),\n",
    "    z = np.reshape(coords[:,1], (20,20)),\n",
    "    opacity = 0.95   \n",
    ")\n",
    "\n",
    "origin = go.Scatter3d(\n",
    "    x = [0],\n",
    "    y = [0],\n",
    "    z = [0],\n",
    "    mode = 'markers', \n",
    "    marker = dict(color='red')\n",
    ")\n",
    "\n",
    "loss_surface.opacity = 0.9\n",
    "fig = go.Figure(data = [raw3d, fit_plane, origin])\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spin the above plot around.  You will notice that our data lives on the plane 3 dimensions and that the plane passes through the origin.  This is a key theme with linear regression. That is by transforming the input we can often make complex data live on a plane that passes through the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on the Bias Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **bias** term can be crucial for modeling data.  Most software packages will have an option to add a bias term to the model.  This is often implemented by transforming the input data though one has to be careful about adding how the bias term is treated in certain model tuning procedures (e.g., regularization which we will cover shortly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we will explore **Feature Engineering** in much greater detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds100]",
   "language": "python",
   "name": "conda-env-ds100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
