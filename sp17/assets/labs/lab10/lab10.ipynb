{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install -U okpy\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab10.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's lab is about the computational nuts and bolts of Ordinary Least Squares regression and featurization.  For simplicity, we will use synthetic datasets.\n",
    "\n",
    "## Basic practice\n",
    "\n",
    "In this first dataset, the feature columns are `x0`, `x1`, etc, and the response column is `y`.  $n = 1000$ and $p = 10$.  Everything is a real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = pd.read_csv(\"data0.csv\")\n",
    "data0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture that a least-squares regression problem can be cast as:\n",
    "\n",
    "$$\\theta^{\\text{OLS}} = \\arg \\min_{\\theta \\in \\mathbb{R}^p} (Y - X \\theta)^T (Y - X \\theta)$$\n",
    "\n",
    "Here $Y \\in \\mathbb{R}^n$ are the true values we're trying to match, one per datum; $X \\in \\mathbb{R}^{n \\times p}$ are the features, one row per datum; and $\\theta \\in \\mathbb{R}^p$ is the parameter vector for a linear prediction function.  So the output of the $\\arg \\min$ is a *vector* of length $p$.\n",
    "\n",
    "We could solve this with numerical optimization, which we saw briefly in the previous lab.  We can also solve it by a direct computation.  Taking the derivative (technically, the *gradient*) of the minimand with respect to $\\theta$ and setting it equal to 0, we get:\n",
    "\n",
    "$$X^T X \\theta = X^T Y$$\n",
    "\n",
    "The solution to this linear equation in $\\theta$ can be computed in various ways.  Here are a few:\n",
    "\n",
    "1. Inverting the matrix $(X^T X)$ and computing $(X^T X)^{-1} X^T Y$.\n",
    "2. Solving the equation directly for $\\theta$ without inverting $(X^T X)$.  ([Here](https://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Orthogonal_decomposition_methods) are some examples.)\n",
    "3. Minimizing $(Y - X \\theta)^T (Y - X \\theta)$ using numerical optimization.  ([Here](https://en.wikipedia.org/wiki/Iterative_method#Linear_systems) is a list of some of the specialized methods that have been developed.)\n",
    "\n",
    "We'll look at methods 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "After loading the data, form the matrix $X$, which should be an $n$ by $p$ NumPy array.  **Also** form the array $Y$, which should be a NumPy array with dimension $n$.\n",
    "\n",
    "*Hint:* The DataFrame method `as_matrix` will be useful.\n",
    "\n",
    "*Hint 2:* If `x` is a DataFrame, then `x[x.columns.difference(['col', 'another_col'])]` is a copy of `x` without the two named columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ...\n",
    "Y = ...\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q1')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Form the matrix $X^T X$.  It should be a $p$ by $p$ NumPy array.  Name the result `XtX`.\n",
    "\n",
    "*Hint:* The operator `@` will multiply two matrices.  You can also use `a.dot(b)`.\n",
    "\n",
    "*Hint 2:* `X.T` is the transpose of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtX = ...\n",
    "XtX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q2')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Invert the matrix `XtX` and use it to solve the normal equations for $\\theta$.  Name the result `theta3`.\n",
    "\n",
    "*Hint:* Import the `scipy` module to do linear algebra.  It is customary to import it as `sc`.  Then use the function `sc.linalg.inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sc\n",
    "XtXinv = ...\n",
    "theta3 = ...\n",
    "theta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q3')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "What is the squared-error loss (the average of the squares of the residuals) for your `theta3` on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_squared_loss4 = np.mean((X @ theta3 - Y)**2)\n",
    "avg_squared_loss4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q4')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "Use `sc.linalg.solve` to solve for $\\theta$ without inverting $X^T X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta5 = sc.linalg.solve(XtX, X.T @ Y)\n",
    "theta5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q5')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "Are the differences between `theta5` and `theta3` large?  Investigate by computing the absolute differences `theta5 - theta3` and the relative differences `(theta5 - theta3) / theta5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "absolute_diffs = ...\n",
    "relative_diffs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see histograms of the differences.\n",
    "# You should see that they are not very large.\n",
    "plt.hist(absolute_diffs)\n",
    "plt.show()\n",
    "plt.hist(relative_diffs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "Use `scikit-learn` to find $\\theta$, as follows:\n",
    "* Import the module `sklearn.linear_model`.\n",
    "* Call `LinearRegression` to produce a least-squares linear regression model.  Be sure to use settings that will produce the same model as we've been using.  At least 1 default option is incorrect!\n",
    "* Call the `fit` method of the model to fit it to $X$ and $Y$.\n",
    "* The `coef_` field of the model will now contain $\\theta$.\n",
    "\n",
    "Use the names provided in the skeleton.\n",
    "\n",
    "*Hint:*  Remember, if you type \"`linear_model.LinearRegression(`\" and then hit Shift+Tab, you'll see the documentation for the `LinearRegression` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lin_model7 = ...\n",
    "...\n",
    "theta7 = ...\n",
    "theta7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q7')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "Using the `predict` method of the model you created, compute a prediction for each row of $X$.  (These should be the same as `X @ theta7`, but it is useful to learn the API.)  Use these predictions to compute the average squared loss as in question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_squared_loss8 = ...\n",
    "avg_squared_loss8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q8')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding and numerical stability\n",
    "The next cell loads a simple, slightly more realistic dataset.\n",
    "\n",
    "We are trying to predict a person's age on the basis of their height and name.  We have a dataset of 5 people.  (This sounds silly, but given a larger dataset, it could be a reasonable idea.  Names [go in and out of fashion over time](https://www.kaggle.com/kaggle/us-baby-names).  For example, someone named Sierra in the United States is likely born in the 1990s, while someone named Judith is likely born in the 1940s.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.DataFrame({\n",
    "        'name':        ['Aditya', 'Basil', 'Caty', 'Delilah', 'Delilah'],\n",
    "        'height (ft)': [5,        5,       2,      4,         4        ],\n",
    "        'age (yrs)':   [29,       17,      3,      7,         12       ]\n",
    "    })\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if we attempt to run something like:\n",
    "\n",
    "    X_people = people[['name', 'height (ft)']].as_matrix()\n",
    "    Y_people = people['age (yrs)'].as_matrix()\n",
    "    sc.linalg.solve(X_people.T @ X_people, X_people.T @ Y_people)\n",
    "\n",
    "...it won't work, because some entries of `X_people` are strings.\n",
    "\n",
    "Recall that a typical solution to this is *one-hot encoding*: we transform the strings into several 0/1 features, with one feature for each string in the dataset.  The package `sklearn.feature_extraction` provides many kinds of featurization, including `sklearn.feature_extraction.DictVectorizer`, which one-hot encodes strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "Use `DictVectorizer` to one-hot encode the `name` column of `people`.  **Then,** create a feature matrix `X_people` containing both the `height (ft)` column and the one-hot-encoded name features.  **Also** create a response matrix `Y_people` containing the ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and examine its output.  Use it to complete\n",
    "# the next cell.\n",
    "from sklearn import feature_extraction\n",
    "one_hot_encoder = feature_extraction.DictVectorizer()\n",
    "list_of_name_dicts = people[['name']].to_dict(orient='records')\n",
    "names_encoded = one_hot_encoder.fit_transform(list_of_name_dicts).toarray()\n",
    "names_encoded_tbl = pd.DataFrame(names_encoded, columns=one_hot_encoder.get_feature_names())\n",
    "names_encoded_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "for_assignment_type": "student"
   },
   "outputs": [],
   "source": [
    "...\n",
    "X_people = ...\n",
    "Y_people = ...\n",
    "X_people, Y_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q9')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "Read the following 4 cells, run them, and examine their output.  What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X_people.T @ X_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_people = sc.linalg.inv(X_people.T @ X_people)\n",
    "inv_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_people_via_inv = sc.linalg.inv(X_people.T @ X_people) @ X_people.T @ Y_people\n",
    "theta_people_via_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_people.T @ X_people @ theta_people_via_inv) - (X_people.T @ Y_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature matrix `X_people` is *rank deficient*, meaning its *rank* is lower than it could be because some of its rows or columns are linearly dependent.  Specifically, `X_people` has 5 columns and 5 rows, and the last two rows are equal, so it has rank 4.  That means `X_people.T @ X_people` also has rank 4 and is therefore not invertible.  Unfortunately, `sc.linalg.inv` produces garbage output in such situations.\n",
    "\n",
    "One solution is to try `sc.linalg.solve` or `linear_model.LinearRegression`.  You're welcome to do that.  However, a student proposes a different solution to this problem:\n",
    "\n",
    "> Let's add tiny random numbers (say, on the order of 1e-15) to each entry of `X_people`.  It shouldn't make any qualitative difference in the features, and it will make `X_people` full-rank, since the last two rows won't be exactly the same any more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11\n",
    "Adding small numbers to a matrix is called a *perturbation*.  Try it out.  Produce a matrix called `X_people_perturbed` that's a copy of `X_people`, but with tiny random numbers added to each element.  Then solve the new least-squares problem by inverting `(X_people_perturbed.T @ X_people_perturbed)`.  Call the result `theta_people_perturbed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The noise matrix is provided for you.  It contains\n",
    "# numbers sampled uniformly at random from the interval\n",
    "# [-1e-15, 1e-15].\n",
    "noise = (np.random.rand(*X_people.shape) - .5)*2*1e-15\n",
    "X_people_perturbed = ...\n",
    "theta_people_perturbed = ...\n",
    "theta_people_perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q11')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12\n",
    "Rerun the cell above a few times.  Do you notice anything that says we have (or haven't) solved the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturbing a matrix that is rank-deficient will *technically* make it invertible, but it remains *ill-conditioned*.  That means that computer solvers will work poorly, and the solution doesn't really make sense because it is highly sensitive to small changes in the input data.  (It is *numerically unstable*.)  That is especially true when we use `sc.linalg.inv`, but you would notice a similar problem with `sc.linalg.solve`.\n",
    "\n",
    "Feature matrices in the wild are not often rank deficient, and they can typically be made full-rank by inconsequential changes to feature values, as you've seen here.  However, feature matrices *are* sometimes ill-conditioned.  The phenomenon is not limited to one-hot encoding; it's just convenient to demonstrate in that case.\n",
    "\n",
    "Fortunately, there is an easy way to check whether a matrix is ill-conditioned: its [*condition number*](https://en.wikipedia.org/wiki/Condition_number).  Higher condition numbers are worse. As a rule of thumb, 1e10 is considered a very high condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13\n",
    "Use the function `np.linalg.cond` to compute the condition numbers of `X_people` and of `X_people_perturbed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_people_condition_number = np.linalg.cond(X_people)\n",
    "X_people_perturbed_condition_number = np.linalg.cond(X_people_perturbed)\n",
    "print(\"Condition number for X_people:\\t\\t\\t{:.4e}\".format(X_people_condition_number))\n",
    "print(\"Condition number for X_people_perturbed:\\t{:.4e}\".format(X_people_perturbed_condition_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q13')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soon we will see *regularization*, a method that (among other benefits) can alleviate numerical instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section, you'll investigate how linear regression works on a much larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up\n",
    "Our $X$ matrix has 10,000 entries, which is not particularly large.  Let us see if your computer can handle a much larger dataset: $n = 100,000$ and $p = 1000$.\n",
    "\n",
    "The mathematical simplicity of least-squares linear regression is a big advantage here.  In fact, the main computational difficulties with the next dataset are (1) *getting the data to your computer* and (2) *holding it in memory*.  To alleviate the first problem, we will synthesize the dataset directly, rather than loading it from a file.\n",
    "\n",
    "**After reading the following note, run the next cell to generate your big dataset.**\n",
    "\n",
    "*Note:* If it takes more than 30 seconds to run the next cell, the generated matrix is probably too large to fit in memory on your computer.  (It contains 100 million floating-point numbers, or roughly 800 megabytes of data.)  You will probably have to restart the kernel.  Then, try reducing `p` in the call to `generate_dataset`, say to 500 or 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n, p):\n",
    "    error_magnitude = 10\n",
    "    theta_magnitude = 100\n",
    "    X = np.random.randn(n, p)\n",
    "    e = np.random.randn(n)*error_magnitude\n",
    "    true_theta = (np.random.rand(p)-.5)*2*theta_magnitude\n",
    "    Y = X @ true_theta + e\n",
    "    return (X, Y, true_theta)\n",
    "\n",
    "X_big, Y_big, true_theta_big = generate_dataset(n = 100000, p = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are in the matrix `X_big`, and the response values are in `Y_big`.  Notice also that the true $\\theta$ is stored in `true_theta_big`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14\n",
    "Perform least-squares linear regression for this dataset using any method you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta14 = ...\n",
    "theta14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q14')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15\n",
    "Compare your `theta14` to `true_theta_big`, the true value that generated the data.  There are 1000 differences, so we can't just read them.  Instead, compute a histogram to show the distribution of differences.  (It may be informative to divide the differences by the average magnitude of the elements of `true_theta_big`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "for_assignment_type": "student"
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that most parameters are well-estimated.  Note that this depends on the amount of data -- if we had only 2000 samples, the errors would be much larger.  If you are interested, it would be informative to try that yourself by generating another dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16\n",
    "The function `time` below will time any function you wish to run.  See how long it takes to run the following steps for the big dataset:\n",
    "\n",
    "1. Compute $X^T X$.\n",
    "2. Compute $X^T Y$.\n",
    "3. Having precomputed $X^T X$, compute $(X^T X)^{-1}$.\n",
    "4. Having precomputed $(X^T X)^{-1}$ and $X^T Y$, compute $(X^T X)^{-1} X^T Y$.\n",
    "5. Having precomputed $X^T X$ and $X^T Y$, solve for $\\theta$ (using `sc.linalg.solve`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time(f, count):\n",
    "    \"\"\"Report how long it takes to call f.\n",
    "    \n",
    "    IMPORTANT NOTE:\n",
    "    The count should be chosen judiciously.  Try 1 to start,\n",
    "    just to make sure your function doesn't take forever to\n",
    "    run.  Then try 10, 100, etc, until it takes at least a\n",
    "    second for this function to finish timing.\n",
    "    \n",
    "    Args:\n",
    "      f (function): A function that takes no arguments and\n",
    "                    performs some computation.\n",
    "      count (int): The number of times to call the function.\n",
    "                   If a function is very fast, calling it\n",
    "                   more times will produce a more accurate\n",
    "                   estimate of its speed.\n",
    "    \n",
    "    Returns:\n",
    "      float: The length of time it takes (in seconds) to call\n",
    "             f, on average.\n",
    "    \n",
    "    Example:\n",
    "    >>> time(lambda: 2 + 2, 1000000)\n",
    "    7.233020500279963e-08\n",
    "    \"\"\"\n",
    "    import timeit\n",
    "    return min(timeit.repeat(f, repeat=3, number=count)) / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_big_t_X_big = X_big.T @ X_big # This should be useful.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_big_t_Y_big = X_big.T @ Y_big\n",
    "X_big_t_X_big_inv = sc.linalg.inv(X_big.T @ X_big)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17\n",
    "Solving a linear system is generally thought to be slow.  Based on your timing results, when $n$ is much larger than $p$, which step in linear regression is the slow one?  (Note that a trick can be used to get similar results if $p$ is much larger than $n$!  Numerical methods like stochastic gradient descent can be faster still.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting your assignment\n",
    "If you made a good-faith effort to complete the lab, change `i_finished_the_lab` to `True` in the cell below.  In any case, run the cells below to submit the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_finished_the_lab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ok.grade('qcompleted')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, run this code in your terminal to make a\n",
    "[git commit](https://www.atlassian.com/git/tutorials/saving-changes/git-commit)\n",
    "that saves a snapshot of your changes in `git`. The last line of the cell\n",
    "runs [git push](http://stackoverflow.com/questions/2745076/what-are-the-differences-between-git-commit-and-git-push), which will send your work to your personal Github repo.\n",
    "\n",
    "    # Tell git to commit your changes to this notebook\n",
    "    git add sp17/lab/lab10/lab10.ipynb\n",
    "    \n",
    "    # Tell git to make the commit\n",
    "    git commit -m \"lab10 finished\"\n",
    "    \n",
    "    # Send your updates to your personal private repo\n",
    "    git push origin master"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ds100]",
   "language": "python",
   "name": "conda-env-ds100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

