{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 12: Logistic Regression Using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "!pip install -U okpy\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab12.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's lab, we're going to use logistic regression to classify handwritten digits. You'll learn about logistic / softmax regression and TensorFlow, a popular machine learning library developed by Google.\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is a library typically used to train deep neural networks (DNNs).  DNN learning is just like linear regression or classification, except that we search over a more complicated class of functions, not just linear ones.  DNNs have been popularized by their success in many fields, such as in spam detection, speech recognition, or even in art, such as [Neural Style](https://github.com/anishathalye/neural-style).  They are a building block in many successful applications of machine learning in recent years.\n",
    "\n",
    "Protip: This lab is taken straight from the [TensorFlow tutorials](https://www.tensorflow.org/get_started/mnist/beginners) so if you get stuck, go ahead and reference that page.\n",
    "\n",
    "## Digitize it\n",
    "\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset is comprised of 60,000 handwritten digits from 0-9 (10 total types).  The data are *greyscale pixels* from scans of handwriting.\n",
    "\n",
    "Let's load in and take a peek at the data. The next cell will download and load the data into a variable called `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the dimensions of the data. You'll see that TensorFlow has already split the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist.train.images.shape, mnist.validation.images.shape, mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training example is originally a 28x28 image:\n",
    "\n",
    "![](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "To make it easier for machine learning, the images are flattened out into length-784 vectors.\n",
    "\n",
    "Here's a function to reshape the vector back into a 28x28 image and a function to display one / multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def example_to_image(example):\n",
    "    '''Takes in a length-784 training example and returns a (28, 28) image.'''\n",
    "    return example.reshape((28, 28))\n",
    "\n",
    "def show_images(images, ncols=2, figsize=(10, 7), **kwargs):\n",
    "    \"\"\"\n",
    "    Shows one or more images.\n",
    "    \n",
    "    images: Image or list of images.\n",
    "    \"\"\"\n",
    "    def show_image(image, axis=plt):\n",
    "        plt.imshow(image, **kwargs)\n",
    "        \n",
    "    if not (isinstance(images, list) or isinstance(images, tuple)):\n",
    "        images = [images]\n",
    "    \n",
    "    nrows = math.ceil(len(images) / ncols)\n",
    "    ncols = min(len(images), ncols)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, image in enumerate(images):\n",
    "        axis = plt.subplot2grid(\n",
    "            (nrows, ncols),\n",
    "            (i // ncols,  i % ncols),\n",
    "        )\n",
    "        axis.tick_params(bottom='off', left='off', top='off', right='off',\n",
    "                         labelleft='off', labelbottom='off')\n",
    "        axis.grid(False)\n",
    "        show_image(image, axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Use the provided `example_to_image` and `show_images` function to visualize the training examples given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These indices are the examples you should show from mnist.train.images\n",
    "examples_to_show = np.array([0,  5100, 10200, 15300, 20400, 25500, 30600, 35700, 40800, 45900])\n",
    "\n",
    "# Get the examples from the training data\n",
    "examples = ...\n",
    "\n",
    "# Convert each example into an image\n",
    "images = ...\n",
    "\n",
    "# Call show_images using ncols=5\n",
    "...\n",
    "\n",
    "# We'll print the labels for each of these examples\n",
    "mnist.train.labels[examples_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are more than 2 labels (0 through 9), and the label data are represented in a one-hot encoding. So, the labels have dimension n x 10.  This is different from what we've done before, but it is is a typical strategy for *multiclass* classification.  We will see how our *softmax* loss function incorporates 10-dimensional labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "We've discussed logistic regression at length during lecture. The basic idea is that instead of taking the standard regression equation:\n",
    "\n",
    "$$ f_\\theta(x) = \\theta_1x_1 + ... + \\theta_dx_d + b = \\theta^\\top x + b $$\n",
    "\n",
    "We fit the sigmoid function instead:\n",
    "\n",
    "$$ f_\\theta(x) = s(\\theta_1x_1 + ... + \\theta_dx_d + b) = s(\\theta^\\top x + b) $$\n",
    "\n",
    "Where $$ s(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "The output of $s$ is always a number between 0 and 1, so we can roughly say, \"This example has a 70% chance of being in class 1 and 30% chance of being in class 2, so we'll label it class 1.\"\n",
    "\n",
    "When we have more than one class (say $J$ classes), we instead use the **softmax** function:\n",
    "\n",
    "$$ \\text{softmax}(x)_i = \\frac{e ^ {x_i}}{\\sum_{j=1}^{J} e^{x_j}} $$\n",
    "\n",
    "Which basically means: \"For an example $x$, give each possible class a score, then make sure all the scores add to 1 so we can say this example has a 50% chance of being a 0, 10% of being a 1, 15% of being a 2, etc.\"\n",
    "\n",
    "Then our regression function becomes:\n",
    "\n",
    "$$ f_\\theta(x) = \\text{softmax}(\\theta^\\top x + b) $$\n",
    "\n",
    "It's important to notice that the output of $f_\\theta$ and the input to $\\text{softmax}$ are 10-dimensional.  Since we learn a different score for each class, we need a whole row of parameters for each class.  Think about what that says about the dimensions of $\\theta$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "Let's code this up in TensorFlow. It's easy to implement this after you learn the syntax.\n",
    "\n",
    "Once you learn the basic syntax, you can create much more complicated models in a similar way. TensorFlow also  allows you to use your computer's GPU (graphical processing unit) to train your model, significantly decreasing training time.\n",
    "\n",
    "We're not going to doing very complicated things in TensorFlow today. However, we'll point out where it gives us flexibility that `scikit-learn` doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow operates on variables and relationships between them.  Defining, training, and using a model has a few steps:\n",
    "\n",
    "1. We define variables for every quantity involved in the modeling process.  Some examples: the input to a model, the parameters of the model, any intermediate calculations done by the model, the outputs of the model, and the true labels we want to match.\n",
    "2. We describe the relationships between those variables; for example, multiplying the parameters by the inputs will produce our scores.\n",
    "3. We fill in the inputs and true labels, and we tell TensorFlow to use gradient descent to compute the best parameters.\n",
    "4. We can then fill in new inputs and observe the outputs of the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs\n",
    "We use `tf.placeholder` to specify an input variable. In our case, we want our training data to be an input to the classifier (eg. training points in -> prediction out).\n",
    "\n",
    "The syntax is: `tf.placeholder( type , shape )` where `shape` is the shape of the input, like a NumPy array's shape.\n",
    "\n",
    "For example, `tf.placeholder(tf.int32, [50, 3])` says: \"This input takes in an integer array with 50 examples, 3 dimensions each.\"  Generally we don't hard-code the first dimension, the number of training examples, ourselves.  Instead, we write `tf.placeholder(tf.int32, [None, 3])`, which says: \"This input takes in an integer array with any number of examples, 3 dimensions each.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**:  Create a placeholder called `x` that takes in a `tf.float32` array with any number of examples from the `mnist` dataset.\n",
    "\n",
    "Then, create a placeholder called `y_` that takes in a `tf.float32` array with any number of corresponding labels from the `mnist` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = ...\n",
    "y_ = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Weight and bias vectors are not determined by external input, but will be constantly updated while the gradient descent training process runs. The syntax to create such variables (initializing them to 0) is:\n",
    "\n",
    "`tf.Variable(tf.zeros( shape ))` where `shape` is the shape of the variable, again in NumPy style.\n",
    "\n",
    "Create variables `theta` and `b` corresponding to the weights and bias of our classifier.\n",
    "\n",
    "Remember that our prediction is a length 10 vector, *not* a single value as we have done before. This means that\n",
    "the dimensions of `theta` are *not* `(784, 1)` as usual. Think carefully about the dimensions of `x`, `theta`, `b`, and our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = ...\n",
    "b = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Now, we can implement our classifier.\n",
    "\n",
    "The `tf.nn.softmax(...)` function provides a softmax implementation for us. Instead of using the typical `X @ theta`, we use `tf.matmul(...)`.  Addition via `+` works as normal.\n",
    "\n",
    "Set `y` to the output of the softmax regression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` is a variable now.  Its value will be determined by the inputs `x` and parameters `theta` and `b`.\n",
    "\n",
    "We can implement all sorts of classifiers just by changing parts of the equation above. You just have to know the functional form of the classification function.\n",
    "\n",
    "In order to train our classifier, we need to implement the correct loss function. In class, we saw that the loss function for logistic regression was the negative log probability assigned by the model to the true labels. This translates directly to softmax regression. When there are multiple classes, it is called the *cross-entropy loss*:\n",
    "\n",
    "$$ L_{y}(\\hat{y}) = - \\sum_{j=1}^{J} y_j \\log \\hat{y}_j $$\n",
    "\n",
    "where $ y $ is the one-hot vector of the label and $ \\hat{y} $ is the vector of predicted softmax values.\n",
    "\n",
    "Verify that if we assign probability 1 to the correct label and 0 to the others, then the loss is 0.  It's also useful to verify that if the prediction is incorrect, the loss is greater than 0.\n",
    "\n",
    "Here's the cross entropy loss in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call `cross_entropy` the loss function, but as a Python object it's just another TensorFlow variable.  Its value is a scalar, the number we'd like to minimize by choosing `theta` and `b`.\n",
    "\n",
    "*Note:* Ordinarily you wouldn't have to write out the last two steps; TensorFlow provides [a single function](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/classification#softmax_cross_entropy_with_logits) that produces the cross-entropy loss given just $\\theta^T x + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Now that we have written down our classification pipeline and loss, we need to tell TensorFlow how to run gradient descent.\n",
    "\n",
    "The syntax for this is:\n",
    "\n",
    "    tf.train.GradientDescentOptimizer( learning_rate ).minimize( loss_fn )\n",
    "\n",
    "Here `learning_rate` is the size of the steps we take at each iteration of gradient descent, and `loss_fn` is the variable defining the loss we'd like to minimize.\n",
    "\n",
    "Set `train_step` to the gradient descent rule using `0.5` as the learning rate and the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it!\n",
    "Our variables were containers or placeholders for data, with no numbers yet.  Similarly, `train_step` is a just a *recipe* for optimizing, embodied in a Python object.  We didn't actually do any optimization yet.  But we're ready now.\n",
    "\n",
    "The next cell tells TensorFlow to repeatedly compute `train_step`, filling in batches of 100 images at a time for `x` and `y_`.  This will update `theta` and `b` using stochastic gradient descent for 1000 iterations, using 100 examples per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did we do?\n",
    "\n",
    "Run the next cell to see how your classifier did on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy:\")\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Let's see some examples of your predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EXAMPLES_TO_SHOW = 10\n",
    "\n",
    "corrects = sess.run(correct_prediction, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "correct_i = np.where(corrects)[0][:EXAMPLES_TO_SHOW]\n",
    "\n",
    "print(\"Correct predictions:\")\n",
    "correct_ex = mnist.test.images[correct_i]\n",
    "correct_images = [example_to_image(example) for example in correct_ex]\n",
    "show_images(correct_images, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incorrect_i = np.where(~corrects)[0][:EXAMPLES_TO_SHOW]\n",
    "print(\"Incorrect predictions:\")\n",
    "incorrect_ex = mnist.test.images[incorrect_i]\n",
    "incorrect_images = [example_to_image(example) for example in incorrect_ex]\n",
    "show_images(incorrect_images, 5)\n",
    "\n",
    "print(\"You predicted:\")\n",
    "print(sess.run(tf.argmax(y,1), feed_dict={x: mnist.test.images, y_: mnist.test.labels})[incorrect_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are some of your incorrect predictions are hard for you to guess, too!\n",
    "\n",
    "We have only scratched the surface of TensorFlow.  If you'd like to continue, you can start at the [online tutorials](https://www.tensorflow.org/versions/r0.10/tutorials/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting your assignment\n",
    "If you made a good-faith effort to complete the lab, change `i_finished_the_lab` to `True` in the cell below.  In any case, run the cells below to submit the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "for_assignment_type": "student"
   },
   "outputs": [],
   "source": [
    "i_finished_the_lab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = ok.grade('qcompleted')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run this code in your terminal to make a\n",
    "[git commit](https://www.atlassian.com/git/tutorials/saving-changes/git-commit)\n",
    "that saves a snapshot of your changes in `git`. The last line of the cell\n",
    "runs [git push](http://stackoverflow.com/questions/2745076/what-are-the-differences-between-git-commit-and-git-push), which will send your work to your personal Github repo.\n",
    "\n",
    "    # Tell git to commit your changes to this notebook\n",
    "    git add -A\n",
    "    \n",
    "    # Tell git to make the commit\n",
    "    git commit -m \"lab11 finished\"\n",
    "    \n",
    "    # Send your updates to your personal private repo\n",
    "    git push origin master"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

